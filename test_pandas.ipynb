{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT project\n",
    "        FROM image_analyses_per_plate\n",
    "        GROUP BY project\n",
    "        ORDER BY project \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in pandas dataframe\n",
    "df_projects = pd.read_sql_query(query, db_uri)\n",
    "\n",
    "display(df_projects.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NameContains = 'AROS-Reproducibility-MoA'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM image_analyses_per_plate\n",
    "        WHERE project LIKE '{NameContains}%%'\n",
    "        AND meta->>'type' = 'cp-qc'\n",
    "        AND analysis_date IS NOT NULL\n",
    "        ORDER BY plate_barcode \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in pandas dataframe\n",
    "df_cp_results = pd.read_sql_query(query, db_uri)\n",
    "\n",
    "display(df_cp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_cp_results.duplicated(subset='plate_barcode', keep=False)\n",
    "duplicates_count = duplicates.sum()\n",
    "\n",
    "# If there are duplicates, display a warning to the user\n",
    "if duplicates_count > 0:\n",
    "    print(f\"Warning: There are {duplicates_count} duplicated rows based on 'plate_barcode'. Please check your data.\")\n",
    "\n",
    "display(df_cp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NameContains = 'AROS-Reproducibility-MoA'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM image_analyses_per_plate\n",
    "        WHERE project LIKE '{NameContains}%%'\n",
    "        AND meta->>'type' = 'cp-qc'\n",
    "        AND analysis_date IS NOT NULL\n",
    "        ORDER BY plate_barcode \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in pandas dataframe\n",
    "df_cp_results = pd.read_sql_query(query, db_uri)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_cp_results[df_cp_results.duplicated(subset='plate_barcode', keep=False)]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    # Group the duplicated data by 'plate_barcode' and count the occurrences\n",
    "    grouped_duplicates = duplicates.groupby('plate_barcode')\n",
    "    for name, group in grouped_duplicates:\n",
    "        print(f\"The plate with barcode {name} is replicated {len(group)} times with analysis_id of {group['analysis_id'].tolist()}\")\n",
    "\n",
    "display(df_cp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some rows if that is needed on Analysis ID\n",
    "df_cp_results = df_cp_results[~df_cp_results.analysis_id.isin([3241])] \n",
    "# These analyses were run on wrong channel map. 2641 contains 3456 rows instead of 2772 since it is a JUMP plate.\n",
    "\n",
    "# reindex df\n",
    "df_cp_results = df_cp_results.reset_index(drop=True)\n",
    "\n",
    "display(df_cp_results)\n",
    "\n",
    "# add cp-result file column\n",
    "df_cp_results['qc-file'] = df_cp_results['results'] + 'qcRAW_images_' + df_cp_results['plate_barcode'] + '.parquet'\n",
    "\n",
    "# read all csv and concat them all into one dataframe\n",
    "df_all_files = pd.DataFrame()\n",
    "for index, row in df_cp_results.iterrows():\n",
    "\n",
    "    df_data_from_one_file =  pd.read_parquet(row['qc-file'])\n",
    "    \n",
    "    # Add column and update barcode (as a workaround It should be included in cellprofiler result in future)\n",
    "    #\n",
    "    df_data_from_one_file['Metadata_AcqID'] = row['plate_acq_id']\n",
    "    df_data_from_one_file['Metadata_Barcode'] = row['plate_barcode']\n",
    "    \n",
    "    print (f'df_data_from_one_file no: {index} contains {df_data_from_one_file.shape[1]} columns and {df_data_from_one_file.shape[0]} rows. name: {row[\"qc-file\"]}')\n",
    "    \n",
    "    df_all_files = pd.concat([df_all_files, df_data_from_one_file])\n",
    "    \n",
    "display(df_all_files.head(2))\n",
    "\n",
    "df_all_files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some columns\n",
    "df_data = df_all_files.copy()\n",
    "df_data['Metadata_AcqID'] = df_data['Metadata_AcqID'].astype(int).astype(str)\n",
    "df_data['Metadata_Site'] = df_data['Metadata_Site'].astype(int).astype(str)\n",
    "df_data['ImageID'] = df_data['Metadata_AcqID'] + '_' + df_data['Metadata_Well'] + '_' + df_data['Metadata_Site']\n",
    "df_data['barcode'] = df_data['Metadata_Barcode']\n",
    "df_data['well_id'] = df_data['Metadata_Well']\n",
    "df_data['plate'] = df_data['Metadata_Barcode']\n",
    "df_data['plate-name'] = df_data['Metadata_Barcode']\n",
    "df_data['plateWell'] = df_data['Metadata_Barcode'] + '_' + df_data['Metadata_Well']\n",
    "df_data['site'] = df_data['Metadata_Site']\n",
    "\n",
    "display(df_data.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_all_files.copy()\n",
    "try:\n",
    "    PlateNames = sorted(list(set(data['Metadata_Barcode'])))\n",
    "    print(f'Number of plates: {len(PlateNames)}')\n",
    "    print(PlateNames)\n",
    "    data.sort_values(['Metadata_Barcode','Metadata_Well', 'Metadata_Site'], inplace = True)\n",
    "    data.reset_index(drop=True, inplace = True)\n",
    "except Exception:\n",
    "    print('Plate names not specified')\n",
    "    PlateNames = []\n",
    "Wells = sorted(list(set(data['Metadata_Well'])))\n",
    "NrOfWells = len(Wells)\n",
    "print(f'Number of wells: {NrOfWells}')\n",
    "\n",
    "\n",
    "Rows = sorted(list({w[0] for w in Wells}))\n",
    "print(*Rows)\n",
    "NrOfRows = len(Rows)\n",
    "Columns = sorted(list({w[1:] for w in Wells}))\n",
    "NrOfColumns = len(Columns)\n",
    "print(*Columns)\n",
    "\n",
    "AllWells = [(x+y) for x in Rows for y in Columns]  #ADDED THIS LINE\n",
    "\n",
    "Sites = sorted(list(set(data['Metadata_Site'])))\n",
    "NrOfSites = len(Sites)\n",
    "print(f'Number of sites: {NrOfSites}')\n",
    "print(\n",
    "    f'Processed {data.shape[0]} of {len(PlateNames) * NrOfWells * NrOfSites} images'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "image_quality_cols = [col for col in data.columns if \"ImageQuality_\" in col]\n",
    "image_quality_module = [col.replace('ImageQuality_', '') for col in image_quality_cols]\n",
    "image_quality_measures = sorted({re.sub('_.*', '', s) for s in image_quality_module})\n",
    "count_measures = len(image_quality_measures)\n",
    "\n",
    "print(f'Image Quality module has measured {count_measures} parameters: {\", \".join(image_quality_measures)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_so_useful = ['TotalArea', 'Scaling', 'TotalIntensity', 'Correlation', 'PercentMinimal',\n",
    "                 'LocalFocusScore', 'MinIntensity', 'MedianIntensity', 'MADIntensity',\n",
    "                 'ThresholdMoG', 'ThresholdBackground', 'ThresholdKapur', 'ThresholdMCT',\n",
    "                 'ThresholdOtsu', 'ThresholdRidlerCalvard', 'ThresholdRobustBackground',\n",
    "                 'PercentMaximal']\n",
    "\n",
    "image_quality_measures = [measure for measure in image_quality_measures if measure not in not_so_useful]\n",
    "count_measures = len(image_quality_measures)\n",
    "\n",
    "print(f'I will use {count_measures} parameters: {\", \".join(image_quality_measures)}')\n",
    "\n",
    "data_frame_dictionary = {measure: data[[col for col in image_quality_cols if f'_{measure}' in col]] for measure in image_quality_measures}\n",
    "data_frame_list = sorted(list(data_frame_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrSubplots=len(data_frame_list)\n",
    "ChannelNames = [\n",
    "    re.sub('.*_', '', c)\n",
    "    for c in list(data_frame_dictionary[data_frame_list[0]].columns)\n",
    "]\n",
    "ChannelNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of measures to not keep\n",
    "not_so_useful_set = {\n",
    "    'TotalArea',\n",
    "    'Scaling',\n",
    "    'TotalIntensity',\n",
    "    'Correlation',\n",
    "    'PercentMinimal',\n",
    "    'LocalFocusScore',\n",
    "    'MinIntensity',\n",
    "    'MedianIntensity',\n",
    "    'MADIntensity',\n",
    "    'ThresholdMoG',\n",
    "    'ThresholdBackground',\n",
    "    'ThresholdKapur',\n",
    "    'ThresholdMCT',\n",
    "    'ThresholdOtsu',\n",
    "    'ThresholdRidlerCalvard',\n",
    "    'ThresholdRobustBackground',\n",
    "    'PercentMaximal',\n",
    "}\n",
    "\n",
    "# Use pandas string operations to filter and transform column names\n",
    "image_quality_cols = data.columns[data.columns.str.startswith('ImageQuality_')]\n",
    "image_quality_measures_all = image_quality_cols.str.replace('ImageQuality_', '').str.split('_').str[0].unique()\n",
    "\n",
    "print(f'Image Quality module has measured {len(image_quality_measures_all)} parameters: {\", \".join(image_quality_measures_all)}')\n",
    "\n",
    "# Use pandas to filter out the not so useful measures\n",
    "image_quality_measures_filtered = image_quality_measures_all[~image_quality_measures_all.isin(not_so_useful_set)]\n",
    "print(f'I will use {len(image_quality_measures_filtered)} parameters: {\", \".join(image_quality_measures_filtered)}')\n",
    "\n",
    "# Filter data to only the necessary columns\n",
    "filtered_data = data[image_quality_cols]\n",
    "\n",
    "# Create the DataFrame dictionary\n",
    "data_frame_dictionary = {measure: filtered_data[filtered_data.columns[filtered_data.columns.str.contains(f'_{measure}')]] for measure in image_quality_measures_filtered}\n",
    "data_frame_list = sorted(data_frame_dictionary.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of measures to keep\n",
    "useful_measures = {\n",
    "    'FocusScore',\n",
    "    'MaxIntensity',\n",
    "    'MeanIntensity',\n",
    "    'PowerLogLogSlope',\n",
    "    'StdIntensity',\n",
    "}\n",
    "\n",
    "# Use pandas string operations to filter and transform column names\n",
    "image_quality_cols = data.columns[data.columns.str.startswith('ImageQuality_')]\n",
    "image_quality_measures_all = image_quality_cols.str.replace('ImageQuality_', '').str.split('_').str[0].unique()\n",
    "\n",
    "print(f'Image Quality module has measured {len(image_quality_measures_all)} parameters: {\", \".join(image_quality_measures_all)}')\n",
    "\n",
    "# Use pandas to filter the measures to the useful ones\n",
    "image_quality_measures_filtered = image_quality_measures_all[image_quality_measures_all.isin(useful_measures)]\n",
    "print(f'I will use {len(image_quality_measures_filtered)} parameters: {\", \".join(image_quality_measures_filtered)}')\n",
    "\n",
    "# Filter data to only the necessary columns\n",
    "filtered_data = data[image_quality_cols]\n",
    "\n",
    "# Create the DataFrame dictionary\n",
    "data_frame_dictionary = {measure: filtered_data[filtered_data.columns[filtered_data.columns.str.contains(f'_{measure}')]] for measure in image_quality_measures_filtered}\n",
    "data_frame_list = sorted(data_frame_dictionary.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration parameters\n",
    "colors = ['darkorange', 'cornflowerblue', 'forestgreen', 'red', 'yellow']\n",
    "alpha = 0.5  # transparency\n",
    "line_width = 0.5\n",
    "figure_size = (10, 5)\n",
    "resolution = 300\n",
    "font_size = 12\n",
    "num_subplots = len(data_frame_list)\n",
    "\n",
    "fig = plt.figure(figsize=(figure_size[0], 1.5 * num_subplots))\n",
    "fig.suptitle(NameContains, fontsize=font_size*1.2, x=0.2)\n",
    "\n",
    "for i in range(num_subplots):\n",
    "    # Extract the current DataFrame and its name\n",
    "    current_df = data_frame_dictionary[data_frame_list[i]]\n",
    "    current_df_name = image_quality_measures[i]\n",
    "\n",
    "    # Set subplot settings\n",
    "    ax = fig.add_subplot(num_subplots, 1, i+1)\n",
    "    ax.set_title(current_df_name, fontsize=font_size)\n",
    "    ax.set_facecolor('w')\n",
    "    ax.spines['bottom'].set_color('w')\n",
    "    ax.spines['top'].set_color('w')\n",
    "    ax.spines['left'].set_color('lightgrey')\n",
    "    ax.spines['right'].set_color('lightgrey')\n",
    "    ax.set_xlim([0, len(current_df.index)])\n",
    "    ax.grid(visible=True, which='major', axis='x', color='lightgrey', linestyle='-', linewidth=1, alpha=1)\n",
    "    ax.grid(visible=True, which='major', axis='y', color='lightgrey', linestyle='', linewidth=0)\n",
    "    ax.tick_params(axis='x', labelbottom=i == num_subplots - 1)\n",
    "\n",
    "    # Plot the data\n",
    "    current_df.plot(kind='line', linewidth=line_width, alpha=alpha, ax=ax, legend=None, color=colors)\n",
    "\n",
    "    xticks = np.arange(0, len(current_df.index), NrOfSites*NrOfColumns*NrOfRows)\n",
    "    ax.set_xticks(xticks)\n",
    "\n",
    "    if i == num_subplots - 1:\n",
    "        ax.set_xticklabels(PlateNames)\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(font_size - 6)\n",
    "            tick.label1.set_rotation(15)\n",
    "\n",
    "# Adjust subplot layout and set legend\n",
    "plt.subplots_adjust(top=0.85, hspace=0.60)\n",
    "legend = fig.legend(ChannelNames, fontsize=8, frameon=False)\n",
    "for line in legend.get_lines():\n",
    "    line.set_linewidth(5.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Configuration parameters\n",
    "colors = ['darkorange', 'cornflowerblue', 'forestgreen', 'red', 'yellow']\n",
    "figure_size = (10, 5)\n",
    "font_size = 12\n",
    "num_subplots = len(data_frame_list)\n",
    "y_range = [-5.5, 6]\n",
    "\n",
    "fig = plt.figure(figsize=(figure_size[0], 1.5 * num_subplots))\n",
    "fig.suptitle(NameContains, fontsize=font_size*1.2, x=0.2)\n",
    "\n",
    "for i in range(num_subplots):\n",
    "    # Extract the current DataFrame and its name\n",
    "    current_df = data_frame_dictionary[data_frame_list[i]]\n",
    "    current_df_name = image_quality_measures[i]\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    current_df_scaled = pd.DataFrame(scaler.fit_transform(current_df), columns=ChannelNames)\n",
    "\n",
    "    # Set subplot settings\n",
    "    ax = fig.add_subplot(num_subplots, 1, i+1)\n",
    "    ax.set_title(current_df_name + ' Scaled', fontsize=font_size)\n",
    "    ax.set_facecolor('w')\n",
    "    ax.spines['bottom'].set_color('w')\n",
    "    ax.spines['top'].set_color('w')\n",
    "    ax.spines['left'].set_color('lightgrey')\n",
    "    ax.spines['right'].set_color('lightgrey')\n",
    "    ax.set_xlim([0, len(current_df_scaled.index)])\n",
    "    ax.set_ylim(y_range[0], y_range[1])\n",
    "    ax.grid(visible=True, which='major', axis='x', color='lightgrey', linestyle='-', linewidth=1, alpha=1)\n",
    "    ax.grid(visible=True, which='major', axis='y', color='lightgrey', linestyle='-', linewidth=1, alpha=1)\n",
    "    ax.tick_params(axis='x', labelbottom=i == num_subplots - 1)\n",
    "\n",
    "    # Plot the data\n",
    "    current_df_scaled.plot(kind='line', linewidth=0.5, alpha=0.5, ax=ax, legend=None, color=colors)\n",
    "\n",
    "    xticks = np.arange(0, len(current_df_scaled.index), NrOfSites*NrOfColumns*NrOfRows)\n",
    "    yticks = np.arange(start=math.ceil(y_range[0]), stop=y_range[1], step=2)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_yticks(yticks)\n",
    "\n",
    "    if i == num_subplots - 1:\n",
    "        ax.set_xticklabels(PlateNames)\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(font_size - 4)\n",
    "            tick.label1.set_rotation(15)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(6)\n",
    "\n",
    "# Adjust subplot layout and set legend\n",
    "plt.subplots_adjust(top=0.85, hspace=0.60)\n",
    "legend = fig.legend(ChannelNames, fontsize=8, frameon=False)\n",
    "for line in legend.get_lines():\n",
    "    line.set_linewidth(5.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(data_frame_list):\n",
    "        print(i, item)\n",
    "\n",
    "p = input(f'Enter an integer from 0 to {len(data_frame_list) - 1}:')\n",
    "try:\n",
    "        p = int(p)\n",
    "except Exception:\n",
    "        p=0\n",
    "        \n",
    "current_df = data_frame_dictionary.get(data_frame_list[p])\n",
    "current_df_name = image_quality_measures[p]\n",
    "current_df.describe(percentiles =  [.25, .5, .75, .90, .99])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LowerLimitScaled = -4.5   #('-inf') # e.g. -3 for MeanIntensityScaled\n",
    "UpperLimitScaled = 4.5    #('inf')\n",
    "NewFlagSc = 'OutlierScaled' + '_' + data_frame_list[p] + '_' + str(LowerLimitScaled) + '_' + str (UpperLimitScaled)\n",
    "print(f'Outliers will be flagged in column: {NewFlagSc}')\n",
    "Flags = [NewFlagSc]\n",
    "data[NewFlagSc] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "current_df_scaled = pd.DataFrame(scaler.fit_transform(current_df), columns=ChannelNames)\n",
    "\n",
    "CurrentDataFrameOutliersMetadata = data[\n",
    "    (current_df_scaled.values >= UpperLimitScaled).any(1)\n",
    "    | (current_df_scaled.values <= LowerLimitScaled).any(1)\n",
    "][[\"Metadata_Barcode\", \"Metadata_Well\", \"Metadata_Site\"]]\n",
    "\n",
    "CurrentDataFrameOutliersValues = current_df_scaled[\n",
    "    (current_df_scaled.values >= UpperLimitScaled).any(1)\n",
    "    | (current_df_scaled.values <= LowerLimitScaled).any(1)\n",
    "]\n",
    "\n",
    "CurrentDataFrameScaledOutliers = CurrentDataFrameOutliersMetadata.merge(\n",
    "    CurrentDataFrameOutliersValues, left_index=True, right_index=True\n",
    ")\n",
    "\n",
    "print(CurrentDataFrameScaledOutliers.shape[0])\n",
    "CurrentDataFrameScaledOutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers = CurrentDataFrameScaledOutliers.index.values.tolist()\n",
    "data.loc[Outliers,NewFlagSc] = 1\n",
    "print(\n",
    "    f'{CurrentDataFrameScaledOutliers.shape[0]} images flagged in column {NewFlagSc}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LowerLimitScaled = -4.5  # float('-inf')\n",
    "UpperLimitScaled = 4.5  # float('inf')\n",
    "Flags = []\n",
    "for p in range(0, len(data_frame_list)):\n",
    "    CurrentDataFrame = data_frame_dictionary.get(data_frame_list[p])\n",
    "    CurrentDFName = image_quality_measures[p]\n",
    "\n",
    "    x_unscaled = CurrentDataFrame.values\n",
    "    x_scaled = StandardScaler().fit_transform(x_unscaled)\n",
    "    CurrentDataFrameScaled = pd.DataFrame(x_scaled, columns=ChannelNames)\n",
    "\n",
    "    NewFlagSc = (\n",
    "        \"OutlierScaled\"\n",
    "        + \"_\"\n",
    "        + data_frame_list[p]\n",
    "        + \"_\"\n",
    "        + str(LowerLimitScaled)\n",
    "        + \"_\"\n",
    "        + str(UpperLimitScaled)\n",
    "    )\n",
    "    Flags.append(NewFlagSc)\n",
    "    data[NewFlagSc] = 0\n",
    "\n",
    "    CurrentDataFrameOutliersMetadata = data[\n",
    "        (CurrentDataFrameScaled.values >= UpperLimitScaled).any(1)\n",
    "        | (CurrentDataFrameScaled.values <= LowerLimitScaled).any(1)\n",
    "    ][[\"Metadata_Barcode\", \"Metadata_Well\", \"Metadata_Site\"]]\n",
    "    CurrentDataFrameOutliersValues = CurrentDataFrameScaled[\n",
    "        (CurrentDataFrameScaled.values >= UpperLimitScaled).any(1)\n",
    "        | (CurrentDataFrameScaled.values <= LowerLimitScaled).any(1)\n",
    "    ]\n",
    "    CurrentDataFrameScaledOutliers = CurrentDataFrameOutliersMetadata.merge(\n",
    "        CurrentDataFrameOutliersValues, left_index=True, right_index=True\n",
    "    )\n",
    "\n",
    "    Outliers = CurrentDataFrameScaledOutliers.index.values.tolist()\n",
    "    data.loc[Outliers, NewFlagSc] = 1\n",
    "\n",
    "data['Total'] = data[Flags].max(axis = 1)\n",
    "Flags.append('Total')\n",
    "print(data[Flags].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling bounds\n",
    "lower_limit_scaled = -4.5\n",
    "upper_limit_scaled = 4.5\n",
    "\n",
    "# List to store flags\n",
    "flags = []\n",
    "\n",
    "# Iterate over the length of the data frame list\n",
    "for index, df_name in enumerate(data_frame_list):\n",
    "    \n",
    "    # Get the current dataframe from the dictionary\n",
    "    current_dataframe = data_frame_dictionary[df_name]\n",
    "    image_quality = image_quality_measures[index]\n",
    "\n",
    "    # Scale the dataframe values\n",
    "    x_unscaled = current_dataframe.values\n",
    "    x_scaled = StandardScaler().fit_transform(x_unscaled)\n",
    "    current_dataframe_scaled = pd.DataFrame(x_scaled, columns=ChannelNames)\n",
    "\n",
    "    # Create a new flag\n",
    "    new_flag_scaled = f\"OutlierScaled_{df_name}_{lower_limit_scaled}_{upper_limit_scaled}\"\n",
    "    flags.append(new_flag_scaled)\n",
    "    data[new_flag_scaled] = 0\n",
    "\n",
    "    # Create a condition for outliers\n",
    "    is_outlier_condition = (current_dataframe_scaled.values >= upper_limit_scaled).any(1) | (current_dataframe_scaled.values <= lower_limit_scaled).any(1)\n",
    "    \n",
    "    # Extract metadata and outlier values\n",
    "    outliers_metadata = data[is_outlier_condition][[\"Metadata_Barcode\", \"Metadata_Well\", \"Metadata_Site\"]]\n",
    "    outliers_values = current_dataframe_scaled[is_outlier_condition]\n",
    "    \n",
    "    # Merge metadata and outlier values\n",
    "    outliers_data = outliers_metadata.merge(outliers_values, left_index=True, right_index=True)\n",
    "    outliers_indices = outliers_data.index.values.tolist()\n",
    "    \n",
    "    # Update the flag column for outliers\n",
    "    data.loc[outliers_indices, new_flag_scaled] = 1\n",
    "\n",
    "# Add a 'Total' column which is the maximum of all flag columns\n",
    "data['Total'] = data[flags].max(axis=1)\n",
    "\n",
    "# Append 'Total' to the list of flags\n",
    "flags.append('Total')\n",
    "\n",
    "# Print the sum of all flags\n",
    "print(data[flags].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flags = data[['Metadata_Barcode', 'Metadata_AcqID', 'Metadata_Well', 'Metadata_Site', 'Count_nuclei'] + list(data_frame_dictionary[data_frame_list[0]].columns) + Flags]\n",
    "df_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NameContains = 'AROS-Reproducibility-MoA'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM image_analyses_per_plate\n",
    "        WHERE project LIKE '{NameContains}%%'\n",
    "        AND meta->>'type' = 'cp-features'\n",
    "        AND analysis_date IS NOT NULL\n",
    "        ORDER BY plate_acq_id, analysis_id\n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in pandas dataframe\n",
    "df_cp_results = pd.read_sql_query(query, db_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your main dataframe containing the meta data for each plate.\n",
    "df_cp_results = df_cp_results  # insert your DataFrame here\n",
    "\n",
    "# Setting directory name to store output files.\n",
    "OutputDir = 'ImageMeanFeatures'\n",
    "if not os.path.exists(OutputDir):\n",
    "    os.makedirs(OutputDir)\n",
    "\n",
    "# Setting the prefix for output file names.\n",
    "plateNamePrefix = 'ImageMeanPlate'\n",
    "\n",
    "print('Start: ' + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Loop through each plate in the meta data.\n",
    "for index, oneplate_analysis_meta in df_cp_results.iterrows():\n",
    "    print(f'Processing plate: {index + 1} of {len(df_cp_results.index)}')\n",
    "\n",
    "    # Checking if the output file already exists, if so skip this iteration.\n",
    "    one_plate_filename = f'{OutputDir}/{plateNamePrefix}_{oneplate_analysis_meta[\"plate_acq_name\"]}.parquet'\n",
    "    if os.path.exists(one_plate_filename):\n",
    "        print(f'File exists already, skipping this plate: {one_plate_filename}')\n",
    "        continue\n",
    "\n",
    "    # Load and process data for each feature: nuclei, cells and cytoplasm.\n",
    "    feature_names = ['featICF_nuclei', 'featICF_cells', 'featICF_cytoplasm']\n",
    "    df_features = {}  # A dictionary to hold dataframes for each feature.\n",
    "    for feature_name in feature_names:\n",
    "        feature_file = f\"{oneplate_analysis_meta['results']}{feature_name}.parquet\"\n",
    "        print(f'Reading feature file: {feature_file}')\n",
    "        print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        df_feature = pd.read_parquet(feature_file)\n",
    "        df_feature.columns = [f\"{col}_{re.sub('_.*', '', re.sub('featICF_', '', feature_name))}\" for col in df_feature.columns]\n",
    "        df_features[feature_name] = df_feature  # Adding the dataframe to our dictionary.\n",
    "        print(f'Finished reading file. Dataframe shape: {df_feature.shape}')\n",
    "\n",
    "    # Merge the features dataframes.\n",
    "    df = df_features['featICF_nuclei'].merge(df_features['featICF_cells'],\n",
    "                                             left_on=['Metadata_Barcode_nuclei', 'Metadata_Site_nuclei', 'Metadata_Well_nuclei', 'Parent_cells_nuclei'],\n",
    "                                             right_on=['Metadata_Barcode_cells', 'Metadata_Site_cells', 'Metadata_Well_cells', 'ObjectNumber_cells'],  # Update column names\n",
    "                                             suffixes=('_nuclei', '_cells'),\n",
    "                                             how='left')\n",
    "\n",
    "    df = df.merge(df_features['featICF_cytoplasm'],\n",
    "                  left_on=['Metadata_Barcode_nuclei', 'Metadata_Site_nuclei', 'Metadata_Well_nuclei', 'Parent_cells_nuclei'],\n",
    "                  right_on=['Metadata_Barcode_cytoplasm', 'Metadata_Site_cytoplasm', 'Metadata_Well_cytoplasm', 'ObjectNumber_cytoplasm'],  # Update column names\n",
    "                  suffixes=('_nuclei', '_cytoplasm'),\n",
    "                  how='left')\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add plate and barcode information to the dataframe.\n",
    "    df['Metadata_AcqID_nuclei'] = oneplate_analysis_meta['plate_acq_id']\n",
    "    df['Metadata_Barcode_nuclei'] = oneplate_analysis_meta['plate_barcode']\n",
    "\n",
    "    # Renaming the columns.\n",
    "    df.rename(columns={'Metadata_Barcode_nuclei': 'Metadata_Barcode',\n",
    "                       'Metadata_Well_nuclei': 'Metadata_Well',\n",
    "                       'Metadata_Site_nuclei': 'Metadata_Site',\n",
    "                       'Metadata_AcqID_nuclei': 'Metadata_AcqID'}, inplace=True)\n",
    "\n",
    "    # Adding an ImageID column by combining certain fields.\n",
    "    df['ImageID'] = df['Metadata_AcqID'].astype(str) + '_' + df['Metadata_Barcode'] + '_' + df['Metadata_Well'] + '_' + df['Metadata_Site'].astype(str)\n",
    "\n",
    "    # Select numeric columns to calculate the mean.\n",
    "    numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    aggregation_functions = {i: np.nanmean for i in numeric_columns}\n",
    "\n",
    "    print('Starting grouping by image ' + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    groupedbyImage = df.groupby(['ImageID', 'Metadata_Barcode', 'Metadata_Well', 'Metadata_Site', 'Metadata_AcqID'], as_index=False).agg(aggregation_functions)\n",
    "\n",
    "    # Save grouped data to a file.\n",
    "    print('Saving to parquet file ' + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    groupedbyImage.to_parquet(one_plate_filename)\n",
    "    print('Finished saving to parquet file ' + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Combine all plates' data and save to a single file.\n",
    "# Now finally read all one-plate files and concat them into an AllPlates-file\n",
    "groupedbyImageAllPlates = pd.DataFrame()\n",
    "for index, oneplate_analysis_meta in df_cp_results.iterrows(): \n",
    "    one_plate_filename = f'{ OutputDir }/{plateNamePrefix}_{ oneplate_analysis_meta[\"plate_acq_name\"] }.parquet'\n",
    "    print(f'read file: {one_plate_filename}')\n",
    "    df = pd.read_parquet(one_plate_filename)\n",
    "    groupedbyImageAllPlates = pd.concat([groupedbyImageAllPlates, df])\n",
    "    \n",
    "all_plates_outfile = f'{OutputDir}/{plateNamePrefix}AllPlates.parquet'\n",
    "groupedbyImageAllPlates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = groupedbyImageAllPlates[groupedbyImageAllPlates['ObjectNumber_nuclei'] >= 10] # drop images that have less than 10 cells\n",
    "print(\"df after dropping low nuclei instances\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir = 'Boxplots_mean_2023_04_27_P013725_only_MarisCopy_MartinCheck'\n",
    "if not os.path.exists(OutputDir):\n",
    "    os.makedirs(OutputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM plate_v1\n",
    "        WHERE layout_id ilike '%%sarscov2%%'\n",
    "        \"\"\"\n",
    "df_plates = pd.read_sql_query(query, db_uri)\n",
    "df_plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "\n",
    "# Create a list of barcodes you want to filter on\n",
    "barcode_list = df_cp_results.plate_barcode.to_list()\n",
    "\n",
    "# Convert the barcode list to a comma-separated string for SQL\n",
    "barcode_str = ', '.join([f\"'{item}'\" for item in barcode_list])\n",
    "\n",
    "NameContain = 'AROS'\n",
    "\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM plate_v1\n",
    "        WHERE layout_id ILIKE '%%{NameContain}%%'\n",
    "        AND barcode IN ({barcode_str})\n",
    "        \"\"\"\n",
    "df_plates = pd.read_sql_query(query, db_uri)\n",
    "df_plates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plates.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "\n",
    "# Example barcode list\n",
    "barcode_list = df_cp_results.plate_barcode.to_list()\n",
    "\n",
    "NameContain = 'AROS'\n",
    "conditions = [f\"layout_id ILIKE '%%{NameContain}%%{barcode}%%'\" for barcode in barcode_list]\n",
    "\n",
    "# Join conditions with 'OR' to create the final condition for WHERE clause\n",
    "where_clause = \" OR \".join(conditions)\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM plate_v1\n",
    "    WHERE {where_clause}\n",
    "\"\"\"\n",
    "\n",
    "df_plates = pd.read_sql_query(query, db_uri)\n",
    "df_plates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed the barcode list section because it's not needed anymore\n",
    "\n",
    "NameContain = 'AROS'\n",
    "\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM plate_v1\n",
    "        WHERE layout_id ILIKE '%%{NameContain}%%'\n",
    "        AND barcode <> ''\n",
    "        \"\"\"\n",
    "df_plates = pd.read_sql_query(query, db_uri)\n",
    "df_plates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df, df_plates, how='left', left_on=['Metadata_Barcode','Metadata_Well'], right_on=['barcode','well_id'])\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_plates\", df_plates.shape)\n",
    "print(\"Unique batchid inclusive 6 controls:\", len(df_plates.batchid.unique()))\n",
    "df_plates.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['compound'] = df_merged['batchid']\n",
    "df_merged['concentration'] = df_merged['cmpd_conc']\n",
    "\n",
    "unique_comp_initial = df_merged.compound.unique()\n",
    "print(\"Number of unique compounds\", len(unique_comp_initial))\n",
    "unique_comp_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.dropna(subset = ['compound'], inplace=True)\n",
    "df_merged.reset_index(drop=True, inplace=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2 = df_merged.merge(df_flags, left_on = ['Metadata_AcqID', 'Metadata_Barcode', 'Metadata_Well', 'Metadata_Site'], right_on = ['Metadata_AcqID', 'Metadata_Barcode', 'Metadata_Well', 'Metadata_Site'], how = 'left')\n",
    "print(\"df_merged2:\", df_merged2.shape)\n",
    "\n",
    "df_merged3 = df_merged2[df_merged2['Total'] == 0 ].copy()\n",
    "print(\"df_merged3:\", df_merged3.shape)\n",
    "\n",
    "print(\"Reduction by\", (len(df_merged2))-(len(df_merged3)) )\n",
    "print(\"Number of flagged instances in QC was\", len(df_flags[df_flags['Total'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged3_nan = df_merged3[df_merged3['Total'].isnull() ]\n",
    "print(\"There are\", len(df_merged3_nan), \"NaN instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged3.cbkid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged3.pert_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poscon = df_merged3[df_merged3['pert_type'] == 'poscon']\n",
    "poscon.batchid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negcon = df_merged3[df_merged3['pert_type'] == 'negcon']\n",
    "negcon.batchid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
