{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT project\n",
    "        FROM image_analyses_per_plate\n",
    "        GROUP BY project\n",
    "        ORDER BY project \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in Polars dataframe\n",
    "df_projects = pl.read_database(query, db_uri)\n",
    "\n",
    "df_projects.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from collections import Counter\n",
    "\n",
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "\n",
    "NameContains = 'AROS-'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM image_analyses_per_plate\n",
    "        WHERE project ILIKE '%%{NameContains}%%'\n",
    "        AND meta->>'type' = 'cp-qc'\n",
    "        AND analysis_date IS NOT NULL\n",
    "        ORDER BY plate_barcode \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in Polars dataframe\n",
    "df_cp_results = pl.read_database(query, db_uri)\n",
    "\n",
    "# df_cp_results['analysis_id'].to_list()\n",
    "# counter = Counter(df_cp_results['plate_barcode'].to_list())\n",
    "# [item for item, count in counter.items() if count > 1]\n",
    "df_cp_results.unique('project')['project'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# your list\n",
    "my_list = ['apple', 'banana', 'apple', 'pear', 'banana', 'kiwi']\n",
    "\n",
    "# count the occurrences of each item\n",
    "counter = Counter(my_list)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "db_uri = \"postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb\"\n",
    "\n",
    "NameContains = \"AROS-R\"\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM image_analyses_per_plate\n",
    "        WHERE project LIKE '{NameContains}%%'\n",
    "        AND meta->>'type' = 'cp-qc'\n",
    "        AND analysis_date IS NOT NULL\n",
    "        ORDER BY plate_barcode \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in Polars dataframe\n",
    "df_cp_results = pl.read_database(query, db_uri)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_cp_results.filter(pl.col(\"plate_barcode\").is_duplicated())\n",
    "\n",
    "if not duplicates.is_empty():\n",
    "    # Group the duplicated data by 'plate_barcode' and count the occurrences\n",
    "    grouped_duplicates = duplicates.groupby(\"plate_barcode\")\n",
    "    for name, group in grouped_duplicates:\n",
    "        print(\n",
    "            f\"The plate with barcode {name} is replicated {len(group)} times with analysis_id of {group['analysis_id'].to_list()}\"\n",
    "        )\n",
    "\n",
    "df_cp_results.n_unique(\"plate_barcode\")\n",
    "\n",
    "df_cp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the highet analysis_id value of replicated rows\n",
    "df_cp_results.sort(\"analysis_id\", descending=True).unique('plate_barcode', keep='first').sort(\"analysis_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows by analysis_id\n",
    "df_cp_results.filter(~pl.col('analysis_id').is_in([475, 471, 479]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep rows by analysis_id\n",
    "df_cp_results.filter(pl.col('analysis_id').is_in([475, 471, 479]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "def get_file_extension(filename):\n",
    "    \"\"\"Helper function to get file extension\"\"\"\n",
    "    possible_extensions = ['.parquet', '.csv', '.tsv']\n",
    "    for ext in possible_extensions:\n",
    "        full_filename = filename + ext\n",
    "        if os.path.isfile(full_filename):\n",
    "            return ext\n",
    "    print(f'Warning: File {filename} with extensions {possible_extensions} not found.')\n",
    "    return None\n",
    "\n",
    "def read_file(filename, extension):\n",
    "    \"\"\"Helper function to read file based on its extension\"\"\"\n",
    "    if extension == '.parquet':\n",
    "        return pl.read_parquet(filename + extension)\n",
    "    elif extension in ['.csv', '.tsv']:\n",
    "        delimiter = ',' if extension == '.csv' else '\\t'\n",
    "        return pl.read_csv(filename + extension, separator=delimiter)\n",
    "    return None\n",
    "\n",
    "# Filter out rows with specific analysis_id\n",
    "df_filtered_results = df_cp_results.sort(\"analysis_id\", descending=True).unique('plate_barcode', keep='first').sort(\"analysis_id\")\n",
    "\n",
    "# Add qc-file column based on 'results' and 'plate_barcode' columns\n",
    "df_filtered_results = df_filtered_results.with_columns(\n",
    "    (pl.col('results') + 'qcRAW_images_'+ pl.col('plate_barcode')).alias('qc-file')\n",
    ")\n",
    "\n",
    "print(f\"Quality control data of {df_filtered_results.height} plates imported:\\n\")\n",
    "\n",
    "# Read and process all the files in a list, skipping files not found\n",
    "dfs = []\n",
    "for row in df_filtered_results.iter_rows(named=True):\n",
    "    ext = get_file_extension(row['qc-file'])\n",
    "    print(f\"\\t{row['qc-file']}{ext}\")\n",
    "    if ext is not None:\n",
    "        df = read_file(row['qc-file'], ext)\n",
    "        df = df.with_columns(\n",
    "            pl.lit(row['plate_acq_id']).alias('Metadata_AcqID'),\n",
    "            pl.lit(row['plate_barcode']).alias('Metadata_Barcode')\n",
    "        )\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all the dataframes at once\n",
    "df_concatenated_files = pl.concat(dfs, how='vertical')\n",
    "\n",
    "df_concatenated_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "def get_file_extension(filename):\n",
    "    \"\"\"Helper function to get file extension\"\"\"\n",
    "    possible_extensions = ['.parquet', '.csv', '.tsv']\n",
    "    for ext in possible_extensions:\n",
    "        full_filename = filename + ext\n",
    "        if os.path.isfile(full_filename):\n",
    "            return ext\n",
    "    print(f'Warning: File {filename} with extensions {possible_extensions} not found.')\n",
    "    return None\n",
    "\n",
    "def read_file(filename, extension):\n",
    "    \"\"\"Helper function to read file based on its extension\"\"\"\n",
    "    if extension == '.parquet':\n",
    "        return pl.read_parquet(filename + extension)\n",
    "    elif extension in ['.csv', '.tsv']:\n",
    "        delimiter = ',' if extension == '.csv' else '\\t'\n",
    "        return pl.read_csv(filename + extension, delimiter=delimiter)\n",
    "    return None\n",
    "\n",
    "# Filter out rows with specific analysis_id\n",
    "df_filtered_results = df_cp_results.filter(~pl.col('analysis_id').is_in([3241]))\n",
    "\n",
    "# Add qc-file column based on 'results' and 'plate_barcode' columns\n",
    "df_filtered_results = df_filtered_results.with_columns(\n",
    "    (pl.col('results') + 'qcRAW_images_'+ pl.col('plate_barcode')).alias('qc-file')\n",
    ")\n",
    "\n",
    "print(f'Experiment has {df_filtered_results.height} files in its path.\\n')\n",
    "\n",
    "# Read and process all the files in a list, skipping files not found\n",
    "dfs = [\n",
    "    read_file(row['qc-file'], get_file_extension(row['qc-file'])).with_columns(\n",
    "        pl.lit(row['plate_acq_id']).alias('Metadata_AcqID'),\n",
    "        pl.lit(row['plate_barcode']).alias('Metadata_Barcode')\n",
    "    ) \n",
    "    for row in df_filtered_results.iter_rows(named=True) \n",
    "    if get_file_extension(row['qc-file']) is not None\n",
    "]\n",
    "\n",
    "# Concatenate all the dataframes at once\n",
    "df_concatenated_files = pl.concat(dfs, how='vertical')\n",
    "\n",
    "df_concatenated_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some columns\n",
    "df_data = df_concatenated_files.clone()\n",
    "\n",
    "df_data.with_columns(\n",
    "    (pl.col('Metadata_AcqID').cast(pl.Utf8) + '_' + pl.col('Metadata_Well') + '_' + pl.col('Metadata_Site').cast(pl.Utf8)).alias('ImageID')\n",
    ")\n",
    "\n",
    "# df_data['Metadata_AcqID'] = df_data['Metadata_AcqID'].astype(int).astype(str)\n",
    "# df_data['Metadata_Site'] = df_data['Metadata_Site'].astype(int).astype(str)\n",
    "# df_data['ImageID'] = df_data['Metadata_AcqID'] + '_' + df_data['Metadata_Well'] + '_' + df_data['Metadata_Site']\n",
    "# df_data['barcode'] = df_data['Metadata_Barcode']\n",
    "# df_data['well_id'] = df_data['Metadata_Well']\n",
    "# df_data['plate'] = df_data['Metadata_Barcode']\n",
    "# df_data['plate-name'] = df_data['Metadata_Barcode']\n",
    "# df_data['plateWell'] = df_data['Metadata_Barcode'] + '_' + df_data['Metadata_Well']\n",
    "# df_data['site'] = df_data['Metadata_Site']\n",
    "\n",
    "# display(df_data.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = df_concatenated_files.clone()\n",
    "    plate_names = data.select('Metadata_Barcode').unique().sort(by='Metadata_Barcode').to_series().to_list()\n",
    "    print(plate_names)\n",
    "except Exception:\n",
    "    print('Plate names not specified')\n",
    "    plate_names = []\n",
    "\n",
    "data = data.sort(['Metadata_Barcode','Metadata_Well', 'Metadata_Site'])\n",
    "\n",
    "wells = data.select('Metadata_Well').unique().sort(by='Metadata_Well').to_series().to_list()\n",
    "number_of_wells = len(wells)\n",
    "print(f'Number of wells: {number_of_wells}')\n",
    "\n",
    "rows = sorted(list({w[0] for w in wells}))\n",
    "number_of_rows = len(rows)\n",
    "print(*rows)\n",
    "\n",
    "columns = sorted(list({w[1:] for w in wells}))\n",
    "number_of_columns = len(columns)\n",
    "print(*columns)\n",
    "\n",
    "all_wells = [(x+y) for x in rows for y in columns]\n",
    "\n",
    "sites = data.select('Metadata_Site').unique().sort(by='Metadata_Site').to_series().to_list()\n",
    "number_of_sites = len(sites)\n",
    "print(f'Number of sites: {number_of_sites}')\n",
    "\n",
    "total_images = data.shape[0]\n",
    "expected_images = len(plate_names) * number_of_wells * number_of_sites\n",
    "\n",
    "print(f'Processed {total_images} of {expected_images} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Collect columns related to image quality\n",
    "image_quality_cols = [col for col in data.columns if \"ImageQuality_\" in col]\n",
    "\n",
    "# Remove 'ImageQuality_' prefix from column names\n",
    "image_quality_module = [col.replace('ImageQuality_', '') for col in image_quality_cols]\n",
    "\n",
    "# Get unique measures from column names, assuming measure is before first underscore\n",
    "image_quality_measures = sorted({re.sub('_.*', '', measure) for measure in image_quality_module})\n",
    "count_measures = len(image_quality_measures)\n",
    "\n",
    "print(f'Image Quality module has measured {count_measures} parameters: {\", \".join(image_quality_measures)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_so_useful = ['TotalArea', 'Scaling', 'TotalIntensity', 'Correlation', 'PercentMinimal',\n",
    "                 'LocalFocusScore', 'MinIntensity', 'MedianIntensity', 'MADIntensity',\n",
    "                 'ThresholdMoG', 'ThresholdBackground', 'ThresholdKapur', 'ThresholdMCT',\n",
    "                 'ThresholdOtsu', 'ThresholdRidlerCalvard', 'ThresholdRobustBackground',\n",
    "                 'PercentMaximal']\n",
    "\n",
    "image_quality_measures = [measure for measure in image_quality_measures if measure not in not_so_useful]\n",
    "count_measures = len(image_quality_measures)\n",
    "\n",
    "print(f'I will use {count_measures} parameters: {\", \".join(image_quality_measures)}')\n",
    "\n",
    "data_frame_dictionary = {measure: data[[col for col in image_quality_cols if f'_{measure}' in col]] for measure in image_quality_measures}\n",
    "data_frame_list = sorted(list(data_frame_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation, LocalFocusScore, ThresholdMoG, ThresholdOtsu\n",
    "for i in range(len(data_frame_list)):\n",
    "    if len(data_frame_dictionary[data_frame_list[i]].columns) > 5:\n",
    "        print(i+1, data_frame_list[i], len(data_frame_dictionary[data_frame_list[i]].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = [\n",
    "    re.sub('.*_', '', c)\n",
    "    for c in list(data_frame_dictionary[data_frame_list[0]].columns)\n",
    "]\n",
    "channel_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars import\n",
    "import polars as pl\n",
    "\n",
    "# Set of measures to not keep\n",
    "not_so_useful_set = {\n",
    "    'TotalArea',\n",
    "    'Scaling',\n",
    "    'TotalIntensity',\n",
    "    'Correlation',\n",
    "    'PercentMinimal',\n",
    "    'LocalFocusScore',\n",
    "    'MinIntensity',\n",
    "    'MedianIntensity',\n",
    "    'MADIntensity',\n",
    "    'ThresholdMoG',\n",
    "    'ThresholdBackground',\n",
    "    'ThresholdKapur',\n",
    "    'ThresholdMCT',\n",
    "    'ThresholdOtsu',\n",
    "    'ThresholdRidlerCalvard',\n",
    "    'ThresholdRobustBackground',\n",
    "    'PercentMaximal',\n",
    "}\n",
    "\n",
    "# Filter and transform column names\n",
    "image_quality_cols = [col for col in data.columns if col.startswith(\"ImageQuality_\")]\n",
    "image_quality_measures_all = {col.replace('ImageQuality_', '').split('_')[0] for col in image_quality_cols}\n",
    "\n",
    "print(f'Image Quality module has measured {len(image_quality_measures_all)} parameters: {\", \".join(image_quality_measures_all)}')\n",
    "\n",
    "# Filter out the not so useful measures\n",
    "image_quality_measures_filtered = {measure for measure in image_quality_measures_all if measure not in not_so_useful_set}\n",
    "print(f'I will use {len(image_quality_measures_filtered)} parameters: {\", \".join(image_quality_measures_filtered)}')\n",
    "\n",
    "# Create the DataFrame dictionary\n",
    "data_frame_dictionary = {measure: data.select([col for col in image_quality_cols if f'_{measure}' in col]) for measure in image_quality_measures_filtered}\n",
    "data_frame_list = sorted(data_frame_dictionary.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of measures to keep\n",
    "useful_measures = {\n",
    "    'FocusScore',\n",
    "    'MaxIntensity',\n",
    "    'MeanIntensity',\n",
    "    'PowerLogLogSlope',\n",
    "    'StdIntensity',\n",
    "}\n",
    "\n",
    "# Filter and transform column names\n",
    "image_quality_cols = [col for col in data.columns if col.startswith(\"ImageQuality_\")]\n",
    "image_quality_measures_all = {col.replace('ImageQuality_', '').split('_')[0] for col in image_quality_cols}\n",
    "\n",
    "print(f'Image Quality module has measured {len(image_quality_measures_all)} parameters: {\", \".join(image_quality_measures_all)}')\n",
    "\n",
    "# Filter out the not so useful measures\n",
    "image_quality_measures_filtered = {measure for measure in image_quality_measures_all if measure in useful_measures}\n",
    "print(f'I will use {len(image_quality_measures_filtered)} parameters: {\", \".join(image_quality_measures_filtered)}')\n",
    "\n",
    "# Create the DataFrame dictionary\n",
    "data_frame_dictionary = {measure: data.select([col for col in image_quality_cols if f'_{measure}' in col]) for measure in image_quality_measures_filtered}\n",
    "data_frame_list = sorted(data_frame_dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation, LocalFocusScore, ThresholdMoG, ThresholdOtsu\n",
    "\n",
    "from pharmbio.qc import get_qc_data_dict\n",
    "\n",
    "get_qc_data_dict(data, module_to_keep={'Correlation'})['Correlation']\n",
    "\n",
    "[\n",
    "    re.sub('^.*?_.*?_', '', c)\n",
    "    for c in list(get_qc_data_dict(data, module_to_keep={'PowerLogLogSlope'})['PowerLogLogSlope'].columns)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_std_df(df: pl.DataFrame, method=\"standardize\"):\n",
    "    methods = {\n",
    "        \"normalize\": lambda x: (x - x.min()) / (x.max() - x.min()),\n",
    "        \"standardize\": lambda x: (x - x.mean()) / x.std(ddof=1),\n",
    "    }\n",
    "\n",
    "    df = df.select(\n",
    "        [\n",
    "            (\n",
    "                methods[method](df[col])\n",
    "                if df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]\n",
    "                else df[col]\n",
    "            ).alias(col)\n",
    "            for col in df.columns\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "lower_limit_scaled = -4.5\n",
    "upper_limit_scaled = 4.5\n",
    "\n",
    "for image_quality_name in data_frame_list:\n",
    "    # Get the current dataframe from the dictionary\n",
    "    current_dataframe = data_frame_dictionary[image_quality_name]\n",
    "\n",
    "    # Scale the dataframe values\n",
    "    current_dataframe_scaled = norm_std_df(current_dataframe, method=\"standardize\")\n",
    "\n",
    "    # Create a new flag\n",
    "    new_flag_scaled = (\n",
    "        f\"OutlierScaled_{image_quality_name}_{lower_limit_scaled}_{upper_limit_scaled}\"\n",
    "    )\n",
    "    data = data.with_columns(\n",
    "        pl.lit(\n",
    "            [\n",
    "                1 if i == True else 0\n",
    "                for i in current_dataframe_scaled.apply(\n",
    "                    lambda row: any(\n",
    "                        (val < lower_limit_scaled) | (val > upper_limit_scaled)\n",
    "                        for val in row\n",
    "                    )\n",
    "                ).to_series()\n",
    "            ]\n",
    "        ).alias(new_flag_scaled)\n",
    "    )\n",
    "    \n",
    "data = data.with_columns(\n",
    "    pl.max(pl.col([item for item in data.columns if item.startswith('OutlierScaled_')])).alias('total')\n",
    ")\n",
    "\n",
    "print(data.select([item for item in data.columns if item.startswith('OutlierScaled_')] + ['total']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "treshold_dict = {\"MaxIntensity\": (-5, 5), \"StdIntensity\": (-3, 3)}\n",
    "\n",
    "# Set the default treshold\n",
    "default_sd_step = (-4.5, 4.5)\n",
    "\n",
    "# Define dictionary to hold the range for each image_quality_name, defaulting to the above values\n",
    "sd_step_dict = defaultdict(lambda: default_sd_step)\n",
    "\n",
    "for key, value in treshold_dict.items():\n",
    "    sd_step_dict[key] = value\n",
    "\n",
    "\n",
    "for image_quality_name in data_frame_list:\n",
    "    # Get the current dataframe from the dictionary\n",
    "    current_dataframe = data_frame_dictionary[image_quality_name]\n",
    "\n",
    "    # Scale the dataframe values\n",
    "    current_dataframe_scaled = norm_std_df(current_dataframe, method=\"standardize\")\n",
    "\n",
    "    # Get the lower and upper treshold for the current image_quality_name\n",
    "    lower_limit_scaled, upper_limit_scaled = sd_step_dict[image_quality_name]\n",
    "\n",
    "    # Create a new flag\n",
    "    new_flag_scaled = (\n",
    "        f\"OutlierZscore_{image_quality_name}_{lower_limit_scaled}_{upper_limit_scaled}\"\n",
    "    )\n",
    "    data = data.with_columns(\n",
    "        pl.lit(\n",
    "            [\n",
    "                1 if i == True else 0\n",
    "                for i in current_dataframe_scaled.apply(\n",
    "                    lambda row: any(\n",
    "                        (val < lower_limit_scaled) | (val > upper_limit_scaled)\n",
    "                        for val in row\n",
    "                    )\n",
    "                ).to_series()\n",
    "            ]\n",
    "        ).alias(new_flag_scaled)\n",
    "    )\n",
    "\n",
    "data = data.with_columns(\n",
    "    pl.max(\n",
    "        pl.col([item for item in data.columns if item.startswith(\"OutlierZscore_\")])\n",
    "    ).alias(\"total\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    data.select(\n",
    "        [item for item in data.columns if item.startswith(\"OutlierZscore_\")] + [\"total\"]\n",
    "    ).sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_std_df(df: pl.DataFrame, method=\"standardize\"):\n",
    "    methods = {\n",
    "        \"normalize\": lambda x: (x - x.min()) / (x.max() - x.min()),\n",
    "        \"standardize\": lambda x: (x - x.mean()) / x.std(ddof=1),\n",
    "    }\n",
    "\n",
    "    df = df.select(\n",
    "        [\n",
    "            (\n",
    "                methods[method](df[col])\n",
    "                if df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]\n",
    "                else df[col]\n",
    "            ).alias(col)\n",
    "            for col in df.columns\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "lower_limit_scaled = -4.5\n",
    "upper_limit_scaled = 4.5\n",
    "outlier_prefix = \"OutlierZscore_\"\n",
    "\n",
    "for image_quality_name in data_frame_list:\n",
    "    # Get the current dataframe from the dictionary\n",
    "    current_dataframe = data_frame_dictionary[image_quality_name]\n",
    "\n",
    "    # Scale the dataframe values\n",
    "    current_dataframe_scaled = norm_std_df(current_dataframe, method=\"standardize\")\n",
    "\n",
    "    # Create a new flag\n",
    "    new_flag_scaled = f\"{outlier_prefix}{image_quality_name}_{lower_limit_scaled}_{upper_limit_scaled}\"\n",
    "    outliers = [\n",
    "        1 if i == True else 0\n",
    "        for i in current_dataframe_scaled.apply(\n",
    "            lambda row: any(\n",
    "                (val < lower_limit_scaled) | (val > upper_limit_scaled) for val in row\n",
    "            )\n",
    "        ).to_series()\n",
    "    ]\n",
    "    data = data.with_columns(pl.lit(outliers).alias(new_flag_scaled))\n",
    "\n",
    "# Identify columns starting with 'OutlierZscore_'\n",
    "outlier_flaged_columns = [item for item in data.columns if item.startswith(outlier_prefix)]\n",
    "\n",
    "data = data.with_columns(pl.max(pl.col(outlier_flaged_columns)).alias(\"total\"))\n",
    "\n",
    "print(data.select(outlier_flaged_columns + [\"total\"]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Interquartile Range (IQR) method can be applied to either raw or scaled data, and the choice largely depends on the context and objectives of your analysis.\n",
    "\n",
    "Raw Data: Applying the IQR method to raw data can be beneficial when your data is not skewed and you have a good understanding of the data's distribution and scales. In this case, the outliers identified by the IQR method will directly correspond to extreme values in your original data.\n",
    "\n",
    "Scaled Data: If the scales of your different columns vary significantly, it may be beneficial to standardize or normalize your data before applying the IQR method. By scaling the data, you ensure that each column contributes equally to the calculation of the IQR and the identification of outliers. This is particularly useful when you're working with high-dimensional data, where you want to avoid one or two features with large scales dominating the outlier detection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_prefix = \"OutlierIQR_\"\n",
    "quantile_limit = 0.25  # this could be any value between 0 and 0.5\n",
    "multiplier = 1.5 # by decreasing the multiplier, the criteria become more strict \n",
    "\n",
    "for image_quality_name in data_frame_list:\n",
    "    # Get the current dataframe from the dictionary\n",
    "    current_dataframe = data_frame_dictionary[image_quality_name]\n",
    "\n",
    "    # Calculate the lower and upper quantiles\n",
    "    lower_quantile = current_dataframe.quantile(quantile_limit)\n",
    "    upper_quantile = current_dataframe.quantile(1 - quantile_limit)\n",
    "\n",
    "    # Define the IQR and the bounds for outliers\n",
    "    IQR = upper_quantile - lower_quantile\n",
    "    lower_threshold = (lower_quantile - multiplier * IQR).to_numpy().min()\n",
    "    upper_threshold = (upper_quantile + multiplier * IQR).to_numpy().max()\n",
    "    print(lower_threshold, upper_threshold)\n",
    "\n",
    "    # Create a new flag\n",
    "    new_flag_iqr = f\"{outlier_prefix}{image_quality_name}_{lower_threshold}_{upper_threshold}\"\n",
    "    outliers = [\n",
    "        1 if i == True else 0\n",
    "        for i in current_dataframe.apply(\n",
    "            lambda row: any(\n",
    "                (val < lower_threshold) | (val > upper_threshold) for val in row\n",
    "            )\n",
    "        ).to_series()\n",
    "    ]\n",
    "    \n",
    "    data = data.with_columns(pl.lit(outliers).alias(new_flag_iqr))\n",
    "\n",
    "# Identify columns starting with 'OutlierScaled_'\n",
    "outlier_flaged_columns = [item for item in data.columns if item.startswith(outlier_prefix)]\n",
    "\n",
    "data = data.with_columns(pl.max(pl.col(outlier_flaged_columns)).alias(\"total\"))\n",
    "\n",
    "print(data.select(outlier_flaged_columns + [\"total\"]).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_flaged_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "\n",
    "for plate in plate_names:\n",
    "    plate_data = data.filter(pl.col(\"Metadata_Barcode\") == plate)\n",
    "    heatmap_data = []\n",
    "    heatmap_data_annot = []\n",
    "    for row in rows:\n",
    "        heatmap_row = []\n",
    "        heatmap_row_annot = []\n",
    "        for column in columns:\n",
    "            well = row + column\n",
    "            count_nuclei = plate_data.filter(pl.col(\"Metadata_Well\") == well)[\n",
    "                \"Count_nuclei\"\n",
    "            ].to_numpy()\n",
    "\n",
    "            # If the value is NaN, convert it to a specific value (like -1 or 0)\n",
    "            if count_nuclei.size == 0:\n",
    "                well_nuclei_count = (\n",
    "                    0  # Or whatever value you'd like to use for missing data\n",
    "                )\n",
    "            else:\n",
    "                well_nuclei_count = np.mean(count_nuclei).round(decimals=0).astype(int)\n",
    "\n",
    "            heatmap_row.append(well_nuclei_count)\n",
    "            heatmap_row_annot.append(f'{well}: {well_nuclei_count}')\n",
    "        heatmap_data.append(heatmap_row)\n",
    "        heatmap_data_annot.append(heatmap_row_annot)\n",
    "\n",
    "    annotation_text = [[\"\" for _ in range(len(row))] for row in heatmap_data]\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        heatmap_data,\n",
    "        x=[i + 1 for i in range(24)],\n",
    "        y=rows,\n",
    "        annotation_text=annotation_text,\n",
    "        colorscale=\"OrRd\",\n",
    "        hovertext=heatmap_data_annot,\n",
    "        hoverinfo=\"text\",\n",
    "    )\n",
    "    fig.update_layout(title_text=f\"Plate: {plate}\", width=700)\n",
    "    fig.update_xaxes(side=\"bottom\")\n",
    "    fig[\"layout\"][\"yaxis\"][\"autorange\"] = \"reversed\"\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_names = ['P013725', 'P013726']\n",
    "# plate_names = ['P013725']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import plotly.subplots as sp\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of columns for your grid\n",
    "plot_size = 400\n",
    "font_ratio = plot_size/400\n",
    "num_columns = 2\n",
    "num_rows = -(-len(plate_names) // num_columns)  # Ceiling division to get number of rows needed\n",
    "\n",
    "titles = [f\"Count_nuclei for {name} \" for name in plate_names]\n",
    "\n",
    "\n",
    "# Create a subplot with num_rows rows and num_columns columns\n",
    "fig = sp.make_subplots(rows=num_rows, cols=num_columns, subplot_titles=titles,)\n",
    "\n",
    "for index, plate in enumerate(plate_names):\n",
    "    plate_data = data.filter(pl.col('Metadata_Barcode') == plate)\n",
    "    heatmap_data = []\n",
    "    heatmap_data_annot = []\n",
    "    for row in rows:\n",
    "        heatmap_row = []\n",
    "        heatmap_row_annot = []\n",
    "        for column in columns:\n",
    "            well = row + column\n",
    "            count_nuclei = plate_data.filter(pl.col('Metadata_Well') == well)['Count_nuclei'].to_numpy()\n",
    "            \n",
    "            if count_nuclei.size == 0:\n",
    "                well_nuclei_count = 0\n",
    "            else:\n",
    "                well_nuclei_count = np.mean(count_nuclei).round(decimals = 0).astype(int)\n",
    "            \n",
    "            heatmap_row.append(well_nuclei_count)\n",
    "            heatmap_row_annot.append(f'{well}: {well_nuclei_count}')\n",
    "        heatmap_data.append(heatmap_row)\n",
    "        heatmap_data_annot.append(heatmap_row_annot)\n",
    "\n",
    "    # Calculate the subplot row and column indices\n",
    "    subplot_row = index // num_columns + 1\n",
    "    subplot_col = index % num_columns + 1\n",
    "    \n",
    "    heatmap = ff.create_annotated_heatmap(\n",
    "        heatmap_data,\n",
    "        x=[str(i+1) for i in range(24)],\n",
    "        y=rows,\n",
    "        annotation_text=heatmap_data,\n",
    "        colorscale='OrRd',\n",
    "        hovertext=heatmap_data_annot,\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    # Add the heatmap to the subplot\n",
    "    fig.add_trace(heatmap.data[0], row=subplot_row, col=subplot_col)\n",
    "\n",
    "# Update x and y axes properties\n",
    "for i in fig['layout']['annotations']:\n",
    "    i['font'] = dict(size=12*font_ratio)\n",
    "fig.update_xaxes(tickfont=dict(size=10*font_ratio), nticks=48, side='bottom')\n",
    "fig.update_yaxes(autorange=\"reversed\", tickfont=dict(size=10))\n",
    "# fig.update_yaxes(tickfont=dict(size=10*font_ratio))\n",
    "\n",
    "# Add the new lines here to adjust annotation positions\n",
    "for ann in fig.layout.annotations:\n",
    "    ann.update(y=ann.y+0.02)\n",
    "    \n",
    "fig.update_layout(height=plot_size*num_rows, width=plot_size*1.425*num_columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "print(px.colors.qualitative.Plotly)\n",
    "print(px.colors.qualitative.Light24)\n",
    "print(px.colors.sequential.Plasma)\n",
    "print(px.colors.sequential.Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Defining a color list\n",
    "colors = px.colors.qualitative.Set1\n",
    "\n",
    "fig = sp.make_subplots(rows=len(image_quality_measures), cols=1, subplot_titles=image_quality_measures, x_title='Plates')\n",
    "\n",
    "for x in range(len(image_quality_measures)):\n",
    "    CurrentDataFrame = data_frame_dictionary.get(data_frame_list[x])\n",
    "    \n",
    "    min_val = CurrentDataFrame.min().to_numpy().min()  # minimum of all columns\n",
    "    max_val = CurrentDataFrame.max().to_numpy().max()  # maximum of all columns\n",
    "    for i, column in enumerate(CurrentDataFrame.columns):\n",
    "        channel_name = channel_names[i]\n",
    "        show_in_legend = (x == 0)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[str(j) for j in range(CurrentDataFrame.height)],\n",
    "                y=CurrentDataFrame[column],\n",
    "                mode='lines',\n",
    "                line=dict(width=0.5, color=colors[i % len(colors)]),\n",
    "                showlegend=False,\n",
    "                name=channel_name if not show_in_legend else \"\",\n",
    "                legendgroup=channel_name, \n",
    "            ),\n",
    "            row=x + 1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "    fig.update_xaxes(range=[0, CurrentDataFrame.height], showticklabels=False, row=x+1, col=1)\n",
    "\n",
    "    fig.add_shape(type=\"line\",\n",
    "        xref=\"x\", yref=\"paper\",\n",
    "        x0=CurrentDataFrame.height/2, y0=min_val, x1=CurrentDataFrame.height/2, y1=max_val,\n",
    "        line=dict(\n",
    "            color=\"Black\",\n",
    "            width=1,\n",
    "            dash=\"dashdot\",\n",
    "        ),\n",
    "        row=x + 1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "# Dummy traces for the legend\n",
    "for i, channel_name in enumerate(channel_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],  # these traces won't appear\n",
    "            y=[None],\n",
    "            mode='lines',\n",
    "            line=dict(width=3, color=colors[i % len(colors)]),  # this will be the width in the legend\n",
    "            legendgroup=channel_name,\n",
    "            name=channel_name,  # this will be the name in the legend\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Add main title\n",
    "fig.update_layout(height=1.8*len(image_quality_measures)*100, title_text=NameContains, title_x=0.1, width=1400)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def norm_std_df(df: pl.DataFrame, method='standardize'):\n",
    "    methods = {\n",
    "        'normalize': lambda x: (x - x.min()) / (x.max() - x.min()),\n",
    "        'standardize': lambda x: (x - x.mean()) / x.std(ddof=1)\n",
    "    }\n",
    "    \n",
    "    df = df.select(\n",
    "        [\n",
    "            (\n",
    "                methods[method](df[col])\n",
    "                if df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]\n",
    "                else df[col]\n",
    "            ).alias(col)\n",
    "            for col in df.columns\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def skl_norm_std_df(df: pl.DataFrame, method='standardize'):\n",
    "    if method == 'standardize':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'normalize':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method {method}, expected 'standardize' or 'normalize'.\")\n",
    "\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = [col for col in df.columns if df[col].dtype in [pl.Float32, pl.Float64]]\n",
    "\n",
    "    # Convert numeric columns to pandas DataFrame, scale them, and then convert back to Polars DataFrame\n",
    "    for col in numeric_cols:\n",
    "        pandas_df = df[col].to_pandas()\n",
    "        transformed_col = scaler.fit_transform(pandas_df.values.reshape(-1,1))\n",
    "        transformed_series = pl.Series(col, transformed_col.ravel())\n",
    "        df = df.with_columns(transformed_series)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "norm_std_df(data_frame_dictionary.get(data_frame_list[0]), method='standardize'), skl_norm_std_df(data_frame_dictionary.get(data_frame_list[0]), method='standardize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Defining a color list\n",
    "colors = px.colors.qualitative.Set1\n",
    "\n",
    "fig = sp.make_subplots(rows=len(image_quality_measures), cols=1, subplot_titles=image_quality_measures, x_title='Plates')\n",
    "\n",
    "for x in range(len(image_quality_measures)):\n",
    "    CurrentDataFrame = data_frame_dictionary.get(data_frame_list[x])\n",
    "    CurrentDataFrame = skl_norm_std_df(CurrentDataFrame)   # scaled df\n",
    "    min_val = CurrentDataFrame.min().to_numpy().min()  # minimum of all columns\n",
    "    max_val = CurrentDataFrame.max().to_numpy().max()  # maximum of all columns\n",
    "    for i, column in enumerate(CurrentDataFrame.columns):\n",
    "        channel_name = channel_names[i]\n",
    "        show_in_legend = (x == 0)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[str(j) for j in range(CurrentDataFrame.height)],\n",
    "                y=CurrentDataFrame[column],\n",
    "                mode='lines',\n",
    "                line=dict(width=0.5, color=colors[i % len(colors)]),\n",
    "                showlegend=False,\n",
    "                name=channel_name if not show_in_legend else \"\",\n",
    "                legendgroup=channel_name, \n",
    "            ),\n",
    "            row=x + 1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "    fig.update_xaxes(range=[0, CurrentDataFrame.height], showticklabels=False, row=x+1, col=1)\n",
    "    fig.update_yaxes(range=[-5, 5], row=x+1, col=1)\n",
    "\n",
    "    fig.add_shape(type=\"line\",\n",
    "        xref=\"x\", yref=\"paper\",\n",
    "        x0=CurrentDataFrame.height/2, y0=min_val, x1=CurrentDataFrame.height/2, y1=max_val,\n",
    "        line=dict(\n",
    "            color=\"Black\",\n",
    "            width=1,\n",
    "            dash=\"dashdot\",\n",
    "        ),\n",
    "        row=x + 1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "# Dummy traces for the legend\n",
    "for i, channel_name in enumerate(channel_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],  # these traces won't appear\n",
    "            y=[None],\n",
    "            mode='lines',\n",
    "            line=dict(width=3, color=colors[i % len(colors)]),  # this will be the width in the legend\n",
    "            legendgroup=channel_name,\n",
    "            name=channel_name,  # this will be the name in the legend\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Add main title\n",
    "fig.update_layout(height=1.8*len(image_quality_measures)*100, title_text=NameContains, title_x=0.1, width=1400)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "NameContains = 'AROS-Reproducibility-MoA'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM image_analyses_per_plate\n",
    "        WHERE project LIKE '%%{NameContains}%%'\n",
    "        AND meta->>'type' = 'cp-features'\n",
    "        AND analysis_date IS NOT NULL\n",
    "        ORDER BY plate_acq_id, analysis_id\n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in pandas dataframe\n",
    "df_cp_results = pl.read_database(query, db_uri).filter(pl.col(['plate_barcode']) == 'P013725')\n",
    "df_cp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import polars.selectors as cs\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Function to aggregate data based on operation\n",
    "def aggregate_morphology_data(df, columns_to_aggregate, groupby_columns, aggregation_function='mean'):\n",
    "    grouped = df.lazy().groupby(groupby_columns)\n",
    "    retain_cols = [c for c in df.columns if c not in columns_to_aggregate]\n",
    "    retained_metadata_df = df.lazy().select(retain_cols)\n",
    "\n",
    "    # Aggregate only the desired columns.\n",
    "    agg_exprs = [getattr(pl.col(col), aggregation_function)().alias(col) for col in columns_to_aggregate]\n",
    "    \n",
    "    # Execute the aggregation.\n",
    "    agg_df = grouped.agg(agg_exprs)\n",
    "    agg_df = agg_df.join(retained_metadata_df, on=groupby_columns, how='left')\n",
    "\n",
    "    return agg_df.sort(groupby_columns).collect()\n",
    "\n",
    "\n",
    "saving_dir = Path(\"data\")\n",
    "object_file_names = ['featICF_nuclei', 'featICF_cells', 'featICF_cytoplasm']\n",
    "plate_name_prefix = 'Image_Mean'\n",
    "aggregation_method = {'cell': 'median', 'site': 'median', 'well': 'median', 'plate': 'mean', 'compound': 'mean'}\n",
    "aggregation_level = 'cell'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "saving_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up progress bar for feedback\n",
    "total_iterations = df_cp_results.height * len(object_file_names)\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Processing\")\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "for index, plate_metadata in enumerate(df_cp_results.iter_rows(named=True)):\n",
    "    # Print separator and progress info\n",
    "    separator = \"\\n\" if index else \"\"\n",
    "    print(f\"{separator}{'_'*50}\", flush=True)\n",
    "    print(f'Processing plate {plate_metadata[\"plate_acq_name\"]} ({index + 1} of {df_cp_results.height}):', flush=True)\n",
    "\n",
    "    # Define and check for existing output files\n",
    "    output_filename = f'{saving_dir}/{plate_name_prefix}_{plate_metadata[\"plate_acq_name\"]}.parquet'\n",
    "    if os.path.exists(output_filename):\n",
    "        print(f'File already exists, reading data from: {output_filename}', flush=True)\n",
    "        existing_df = pl.read_parquet(output_filename)\n",
    "        all_dataframes.append(existing_df)\n",
    "        progress_bar.update(len(object_file_names))\n",
    "        continue\n",
    "\n",
    "    # Load and process feature datasets\n",
    "    object_feature_dataframes = {}\n",
    "    unusful_col_pattern = r'^(FileName|PathName|ImageNumber|Number_Object_Number)'\n",
    "    \n",
    "    for object_file_name in object_file_names:\n",
    "        object_feature_file_path = f\"{plate_metadata['results']}{object_file_name}.parquet\"\n",
    "\n",
    "        # Read the parquet file and adjust column names\n",
    "        columns_names = pl.scan_parquet(object_feature_file_path).columns\n",
    "        object_feature_df = pl.read_parquet(object_feature_file_path, columns=[col for col in columns_names if not re.match(unusful_col_pattern, col)]).rename({'ObjectNumber' : 'id'})\n",
    "        object_name = object_file_name.split('_')[-1]\n",
    "        object_feature_df.columns = [f\"{col}_{object_name}\" for col in object_feature_df.columns]\n",
    "\n",
    "        object_feature_dataframes[object_name] = object_feature_df\n",
    "        print(f'\\tReading features {object_feature_df.shape} - {object_name}: \\t{object_feature_file_path}', flush=True)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print('Merging the data', flush=True)\n",
    "    # Join nuclei and cell data on specified columns\n",
    "    df_combined = object_feature_dataframes['cells'].join(\n",
    "        object_feature_dataframes['nuclei'],\n",
    "        left_on=['Metadata_AcqID_cells', 'Metadata_Barcode_cells', 'Metadata_Well_cells', 'Metadata_Site_cells', 'id_cells'],\n",
    "        right_on=['Metadata_AcqID_nuclei', 'Metadata_Barcode_nuclei', 'Metadata_Well_nuclei', 'Metadata_Site_nuclei', 'Parent_cells_nuclei'],\n",
    "        how='left', suffix='_nuclei'\n",
    "    )\n",
    "\n",
    "    # Further join with cytoplasm data on specified columns\n",
    "    df_combined = df_combined.join(\n",
    "        object_feature_dataframes['cytoplasm'],\n",
    "        left_on=['Metadata_AcqID_cells', 'Metadata_Barcode_cells','Metadata_Well_cells', 'Metadata_Site_cells', 'id_cells'],\n",
    "        right_on=['Metadata_AcqID_cytoplasm','Metadata_Barcode_cytoplasm', 'Metadata_Well_cytoplasm', 'Metadata_Site_cytoplasm', 'Parent_cells_cytoplasm'],\n",
    "        how='left', suffix='_cytoplasm'\n",
    "    )\n",
    "\n",
    "    # Renaming columns for better consistency\n",
    "    rename_map = {\n",
    "        'Metadata_AcqID_cells': 'Metadata_AcqID',\n",
    "        'Metadata_Barcode_cells': 'Metadata_Barcode',\n",
    "        'Metadata_Well_cells': 'Metadata_Well',\n",
    "        'Metadata_Site_cells': 'Metadata_Site',\n",
    "        'Children_cytoplasm_Count_cells': 'Cell_cytoplasm_count',\n",
    "        'Children_nuclei_Count_cells': 'Cell_nuclei_count',\n",
    "    }\n",
    "    df_combined = df_combined.rename(rename_map)\n",
    "    \n",
    "    # Create ImageID column by concatenating other columns\n",
    "    image_id = (df_combined['Metadata_AcqID'] + '_' + \n",
    "                df_combined['Metadata_Barcode'] + '_' + \n",
    "                df_combined['Metadata_Well'] + '_' + \n",
    "                df_combined['Metadata_Site']).alias(\"ImageID\")\n",
    "    df_combined = df_combined.with_columns([image_id])\n",
    "    \n",
    "    # # Create CellID column by concatenating other columns\n",
    "    cell_id = (df_combined['ImageID'] + '_' +\n",
    "                df_combined['id_cells']).alias(\"CellID\")\n",
    "    df_combined = df_combined.with_columns([cell_id])\n",
    "\n",
    "    drop_map = [\n",
    "        'Children_cytoplasm_Count_nuclei',\n",
    "        'Parent_precells_cells',\n",
    "        'Parent_nuclei_cytoplasm',\n",
    "        'id_cells',\n",
    "        'id_nuclei',\n",
    "        'id_cytoplasm',      \n",
    "    ]\n",
    "    df_combined = df_combined.drop(drop_map)\n",
    "    \n",
    "    # Ensure data type consistency for certain columns\n",
    "    cast_cols = [\n",
    "        pl.col('Metadata_AcqID').cast(pl.Utf8),\n",
    "        pl.col('Metadata_Site').cast(pl.Utf8),\n",
    "    ]\n",
    "    df_combined = df_combined.with_columns(cast_cols)\n",
    "    \n",
    "    # ordering the columns\n",
    "    \n",
    "    morphology_feature_cols = df_combined.select(cs.by_dtype(pl.NUMERIC_DTYPES)).columns\n",
    "    morphology_feature_cols.remove('Cell_nuclei_count')\n",
    "    morphology_feature_cols.remove('Cell_cytoplasm_count')\n",
    "    non_numeric_cols = df_combined.select(cs.by_dtype(pl.Utf8)).columns\n",
    "    new_order = sorted(non_numeric_cols) + ['Cell_nuclei_count', 'Cell_cytoplasm_count'] + morphology_feature_cols\n",
    "    \n",
    "    df_combined = df_combined.select(new_order)\n",
    "    \n",
    "    barcode_list = df_combined['Metadata_Barcode'].unique().to_list()\n",
    "    barcode_str = ', '.join([f\"'{item}'\" for item in barcode_list])\n",
    "\n",
    "    query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM plate_v1\n",
    "            WHERE (barcode IN ({barcode_str}))\n",
    "            AND batch_id <> ''\n",
    "            \"\"\"\n",
    "    df_plates = pl.read_database(query, db_uri)\n",
    "    \n",
    "    # Join data with df_plates\n",
    "    df_combined = df_combined.join(df_plates, how='left', left_on=['Metadata_Barcode', 'Metadata_Well'], right_on=['barcode', 'well_id'])\n",
    "    df_combined = df_combined.drop_nulls(subset='batch_id')\n",
    "\n",
    "    # Mapping of aggregation levels to their grouping columns\n",
    "    grouping_columns_map = {\n",
    "        'cell': ['CellID', 'ImageID', 'Metadata_AcqID', 'Metadata_Barcode', 'Metadata_Well', 'Metadata_Site', 'batch_id'],\n",
    "        'site': ['ImageID', 'Metadata_AcqID', 'Metadata_Barcode', 'Metadata_Well', 'Metadata_Site', 'batch_id'],\n",
    "        'well': ['Metadata_AcqID', 'Metadata_Barcode', 'Metadata_Well', 'batch_id'],\n",
    "        'plate': ['Metadata_AcqID', 'Metadata_Barcode', 'batch_id'],\n",
    "        'compound': ['batch_id']\n",
    "    }\n",
    "\n",
    "    # Initialize aggregated data with df_combined\n",
    "    aggregated_data = aggregate_morphology_data(\n",
    "            df=df_combined,\n",
    "            columns_to_aggregate=morphology_feature_cols,\n",
    "            groupby_columns=grouping_columns_map['cell'],\n",
    "            aggregation_function=aggregation_method['cell']\n",
    "        )\n",
    "\n",
    "    # Iterate over the levels and aggregate data progressively\n",
    "    if aggregation_level != 'cell':\n",
    "        for level in ['site', 'well', 'plate', 'compound']:\n",
    "            aggregated_data = aggregate_morphology_data(\n",
    "                df=aggregated_data,\n",
    "                columns_to_aggregate=morphology_feature_cols,\n",
    "                groupby_columns=grouping_columns_map[level],\n",
    "                aggregation_function=aggregation_method[level]\n",
    "            )\n",
    "            if aggregation_level == level:\n",
    "                break\n",
    "    \n",
    "    # Write the aggregated data to a parquet file\n",
    "    aggregated_data.write_parquet(output_filename)\n",
    "    all_dataframes.append(aggregated_data)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# After the loop\n",
    "if len(all_dataframes) > 1:\n",
    "    all_plates_df = pl.concat(all_dataframes)\n",
    "    all_plates_df.write_parquet(f\"{saving_dir}/all_plates.parquet\")\n",
    "else:\n",
    "    all_plates_df = all_dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate data based on operation\n",
    "def aggregate_morphology_data(df, columns_to_aggregate, groupby_columns, aggregation_function='mean'):\n",
    "    grouped = df.lazy().groupby(groupby_columns)\n",
    "    retain_cols = [c for c in df.columns if c not in columns_to_aggregate]\n",
    "    retained_metadata_df = df.lazy().select(retain_cols)\n",
    "\n",
    "    # Aggregate only the desired columns.\n",
    "    agg_exprs = [getattr(pl.col(col), aggregation_function)().alias(col) for col in columns_to_aggregate]\n",
    "    \n",
    "    # Execute the aggregation.\n",
    "    agg_df = grouped.agg(agg_exprs)\n",
    "    agg_df = agg_df.join(retained_metadata_df, on=groupby_columns, how='left')\n",
    "\n",
    "    return agg_df.sort(groupby_columns).collect()\n",
    "\n",
    "# # Function to aggregate data based on operation\n",
    "# def aggregate_data(df, columns_to_aggregate, groupby_columns, aggregation_function='mean'):\n",
    "#     pass\n",
    "\n",
    "df_combined.lazy().groupby(grouping_columns_map['cell'], maintain_order=True).median().collect().to_pandas()\n",
    "\n",
    "# aggregate_data(\n",
    "#             df=df_combined,\n",
    "#             columns_to_aggregate=morphology_feature_cols,\n",
    "#             groupby_columns=grouping_columns_map['cell'],\n",
    "#             aggregation_function=aggregation_method['cell']\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data['Metadata_Barcode'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_list = ['P013725']\n",
    "barcode_str = ', '.join([f\"'{item}'\" for item in barcode_list])\n",
    "\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM plate_v1\n",
    "        WHERE (barcode IN ({barcode_str}))\n",
    "        AND batch_id <> ''\n",
    "        \"\"\"\n",
    "pl.read_database(query, db_uri).filter(pl.col('batch_id') is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = aggregated_data.join(df_plates, how='left', left_on=['Metadata_Barcode','Metadata_Well'], right_on=['barcode','well_id'])\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.with_columns(\n",
    "    pl.col('batchid').alias('compound'),\n",
    "    pl.col('cmpd_conc').alias('concentration'),\n",
    "    )\n",
    "df_merged.unique('compound', maintain_order=True)['compound'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop_nulls(subset='compound')\n",
    "df_merged.unique('compound', maintain_order=True)['compound'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
