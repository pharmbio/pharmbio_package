{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT project\n",
    "        FROM image_analyses_per_plate\n",
    "        GROUP BY project\n",
    "        ORDER BY project \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in Polars dataframe\n",
    "df_projects = pl.read_database(query, db_uri)\n",
    "\n",
    "df_projects.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Covid19-Profiling', 'sarscov2-repurposing']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pharmbio equivalant\n",
    "\n",
    "from pharmbio.dataset import get_projects_list\n",
    "\n",
    "get_projects_list(lookup='cov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from collections import Counter\n",
    "\n",
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "\n",
    "NameContains = 'AROS-Reproducibility-MoA'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM image_analyses_per_plate\n",
    "        WHERE project LIKE '{NameContains}%%'\n",
    "        AND meta->>'type' = 'cp-qc'\n",
    "        AND analysis_date IS NOT NULL\n",
    "        ORDER BY plate_barcode \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in Polars dataframe\n",
    "df_cp_results = pl.read_database(query, db_uri)\n",
    "\n",
    "df_cp_results['analysis_id'].to_list()\n",
    "counter = Counter(df_cp_results['plate_barcode'].to_list())\n",
    "[item for item, count in counter.items() if count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quering the db for AROS-CP\n",
      "Analysis for the plate with barcode P009-P012-CACO2 is replicated 5 times with analysis_id of [446, 476, 477, 478, 479]\n",
      "Analysis for the plate with barcode P005-P008-A549 is replicated 4 times with analysis_id of [472, 473, 474, 475]\n",
      "Analysis for the plate with barcode P001-P004-U2OS is replicated 4 times with analysis_id of [468, 469, 470, 471]\n"
     ]
    }
   ],
   "source": [
    "# pharmbio equivalant\n",
    "\n",
    "from pharmbio.dataset import get_projects_list, ExperimentData\n",
    "\n",
    "data = ExperimentData(Name='AROS-CP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>project</th><th>plate_barcode</th><th>plate_acq_name</th><th>plate_acq_id</th><th>analysis_id</th><th>analysis_date</th><th>analysis_error</th><th>meta</th><th>pipeline_name</th><th>results</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i32</td><td>i32</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;AROS-CP&quot;</td><td>&quot;CoP013737-U2OS…</td><td>&quot;CoP013737-U2OS…</td><td>1080</td><td>364</td><td>&quot;2021-06-14&quot;</td><td>null</td><td>&quot;{&quot;type&quot;:&quot;cp-qc…</td><td>&quot;384-96_QC-batc…</td><td>&quot;/share/data/ce…</td></tr><tr><td>&quot;AROS-CP&quot;</td><td>&quot;P001-P004-U2OS…</td><td>&quot;P001-P004-U2OS…</td><td>1152</td><td>471</td><td>&quot;2021-08-01&quot;</td><td>null</td><td>&quot;{&quot;type&quot;:&quot;cp-qc…</td><td>&quot;384-96_QC-batc…</td><td>&quot;/share/data/ce…</td></tr><tr><td>&quot;AROS-CP&quot;</td><td>&quot;P005-P008-A549…</td><td>&quot;P005-P008-A549…</td><td>1140</td><td>475</td><td>&quot;2021-08-01&quot;</td><td>null</td><td>&quot;{&quot;type&quot;:&quot;cp-qc…</td><td>&quot;384-96_QC-batc…</td><td>&quot;/share/data/ce…</td></tr><tr><td>&quot;AROS-CP&quot;</td><td>&quot;P009-P012-CACO…</td><td>&quot;P009-P012-CACO…</td><td>1148</td><td>479</td><td>&quot;2021-08-01&quot;</td><td>null</td><td>&quot;{&quot;type&quot;:&quot;cp-qc…</td><td>&quot;384-96_QC-batc…</td><td>&quot;/share/data/ce…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 10)\n",
       "┌─────────┬────────────┬────────────┬────────────┬───┬────────────┬─────────┬────────────┬─────────┐\n",
       "│ project ┆ plate_barc ┆ plate_acq_ ┆ plate_acq_ ┆ … ┆ analysis_e ┆ meta    ┆ pipeline_n ┆ results │\n",
       "│ ---     ┆ ode        ┆ name       ┆ id         ┆   ┆ rror       ┆ ---     ┆ ame        ┆ ---     │\n",
       "│ str     ┆ ---        ┆ ---        ┆ ---        ┆   ┆ ---        ┆ str     ┆ ---        ┆ str     │\n",
       "│         ┆ str        ┆ str        ┆ i32        ┆   ┆ str        ┆         ┆ str        ┆         │\n",
       "╞═════════╪════════════╪════════════╪════════════╪═══╪════════════╪═════════╪════════════╪═════════╡\n",
       "│ AROS-CP ┆ CoP013737- ┆ CoP013737- ┆ 1080       ┆ … ┆ null       ┆ {\"type\" ┆ 384-96_QC- ┆ /share/ │\n",
       "│         ┆ U2OS-45h-L ┆ U2OS-45h-L ┆            ┆   ┆            ┆ :\"cp-qc ┆ batch1     ┆ data/ce │\n",
       "│         ┆ 1          ┆ 1          ┆            ┆   ┆            ┆ \"}      ┆            ┆ llprofi │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆         ┆            ┆ ler/aut │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆         ┆            ┆ omat…   │\n",
       "│ AROS-CP ┆ P001-P004- ┆ P001-P004- ┆ 1152       ┆ … ┆ null       ┆ {\"type\" ┆ 384-96_QC- ┆ /share/ │\n",
       "│         ┆ U2OS       ┆ U2OS       ┆            ┆   ┆            ┆ :\"cp-qc ┆ batch1     ┆ data/ce │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆ \"}      ┆            ┆ llprofi │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆         ┆            ┆ ler/aut │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆         ┆            ┆ omat…   │\n",
       "│ AROS-CP ┆ P005-P008- ┆ P005-P008- ┆ 1140       ┆ … ┆ null       ┆ {\"type\" ┆ 384-96_QC- ┆ /share/ │\n",
       "│         ┆ A549       ┆ A549       ┆            ┆   ┆            ┆ :\"cp-qc ┆ batch1     ┆ data/ce │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆ \"}      ┆            ┆ llprofi │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆         ┆            ┆ ler/aut │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆         ┆            ┆ omat…   │\n",
       "│ AROS-CP ┆ P009-P012- ┆ P009-P012- ┆ 1148       ┆ … ┆ null       ┆ {\"type\" ┆ 384-96_QC- ┆ /share/ │\n",
       "│         ┆ CACO2      ┆ CACO2      ┆            ┆   ┆            ┆ :\"cp-qc ┆ batch1     ┆ data/ce │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆ \"}      ┆            ┆ llprofi │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆         ┆            ┆ ler/aut │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆         ┆            ┆ omat…   │\n",
       "└─────────┴────────────┴────────────┴────────────┴───┴────────────┴─────────┴────────────┴─────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# your list\n",
    "my_list = ['apple', 'banana', 'apple', 'pear', 'banana', 'kiwi']\n",
    "\n",
    "# count the occurrences of each item\n",
    "counter = Counter(my_list)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "db_uri = 'postgresql://pharmbio_readonly:readonly@imagedb-pg-postgresql.services.svc.cluster.local/imagedb'\n",
    "\n",
    "NameContains = 'AROS-Reproducibility-MoA'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM image_analyses_per_plate\n",
    "        WHERE project LIKE '{NameContains}%%'\n",
    "        AND meta->>'type' = 'cp-qc'\n",
    "        AND analysis_date IS NOT NULL\n",
    "        ORDER BY plate_barcode \n",
    "        \"\"\"\n",
    "\n",
    "# Query database and store result in Polars dataframe\n",
    "df_cp_results = pl.read_database(query, db_uri)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_cp_results.filter(pl.col('plate_barcode').is_duplicated())\n",
    "\n",
    "if not duplicates.is_empty():\n",
    "    # Group the duplicated data by 'plate_barcode' and count the occurrences\n",
    "    grouped_duplicates = duplicates.groupby('plate_barcode')\n",
    "    for name, group in grouped_duplicates:\n",
    "        print(f\"The plate with barcode {name} is replicated {len(group)} times with analysis_id of {group['analysis_id'].to_list()}\")\n",
    "\n",
    "df_cp_results.n_unique('plate_barcode')\n",
    "\n",
    "df_cp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the highet analysis_id value of replicated rows\n",
    "df_cp_results.sort(\"analysis_id\", descending=True).unique('plate_barcode', keep='first').sort(\"analysis_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows by analysis_id\n",
    "df_cp_results.filter(~pl.col('analysis_id').is_in([475, 471, 479]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep rows by analysis_id\n",
    "df_cp_results.filter(pl.col('analysis_id').is_in([475, 471, 479]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data with csv format : AROS-CP\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Add cp-result file column\n",
    "df_cp_results = df_cp_results.with_columns(\n",
    "    pl.lit(df_cp_results['results']+'qcRAW_images_'+ df_cp_results['plate_barcode']+ '.csv').alias('qc-file')\n",
    ")\n",
    "\n",
    "# Read all Parquet files and concatenate them into one DataFrame\n",
    "df_all_files = pl.DataFrame()\n",
    "for idx, row in enumerate(df_cp_results.iter_rows(named=True)):\n",
    "    df_data_from_one_file = pl.read_csv(row['qc-file'])\n",
    "    \n",
    "    # Add column and update barcode\n",
    "    df_data_from_one_file = df_data_from_one_file.with_columns(\n",
    "        pl.lit(row['plate_acq_id']).alias('Metadata_AcqID'),\n",
    "        pl.lit(row['plate_barcode']).alias('Metadata_Barcode')\n",
    "    )\n",
    "    \n",
    "    print(f'df_data_from_one_file no: {idx} contains {df_data_from_one_file.width} columns and {df_data_from_one_file.height} rows. name: {row[\"qc-file\"]}')\n",
    "    \n",
    "    df_all_files = df_all_files.vstack(df_data_from_one_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with parquet format : AROS-Reproducibility-MoA\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Filter out rows with specific analysis_id\n",
    "df_filtered_results = df_cp_results.filter(~pl.col('analysis_id').is_in([3241]))\n",
    "\n",
    "# Add qc-file column based on 'results' and 'plate_barcode' columns\n",
    "df_filtered_results = df_filtered_results.with_columns(\n",
    "    (pl.col('results') + 'qcRAW_images_'+ pl.col('plate_barcode') + '.parquet').alias('qc-file')\n",
    ")\n",
    "\n",
    "\n",
    "print(f'Experiment has {df_filtered_results.height} parquet files in its path.\\n')\n",
    "\n",
    "# Initialize an empty DataFrame to store all the parquet files data\n",
    "df_concatenated_files = pl.DataFrame()\n",
    "\n",
    "for idx, row in enumerate(df_filtered_results.iter_rows(named=True)):\n",
    "    # Read data from the parquet file\n",
    "    df_single_file_data = pl.read_parquet(row['qc-file'])\n",
    "    \n",
    "    # Add 'Metadata_AcqID' and 'Metadata_Barcode' columns\n",
    "    df_single_file_data = df_single_file_data.with_columns(\n",
    "        pl.lit(row['plate_acq_id']).alias('Metadata_AcqID'),\n",
    "        pl.lit(row['plate_barcode']).alias('Metadata_Barcode')\n",
    "    )\n",
    "    \n",
    "    print(f'File {idx + 1} contains {df_single_file_data.width} columns and {df_single_file_data.height} rows. Path: {row[\"qc-file\"]}')\n",
    "    \n",
    "    # Stack the new data onto the previous DataFrame\n",
    "    df_concatenated_files = df_concatenated_files.vstack(df_single_file_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some columns\n",
    "df_data = df_concatenated_files.clone()\n",
    "\n",
    "df_data.with_columns(\n",
    "    (pl.col('Metadata_AcqID').cast(pl.Utf8) + '_' + pl.col('Metadata_Well') + '_' + pl.col('Metadata_Site').cast(pl.Utf8)).alias('ImageID')\n",
    ")\n",
    "\n",
    "# df_data['Metadata_AcqID'] = df_data['Metadata_AcqID'].astype(int).astype(str)\n",
    "# df_data['Metadata_Site'] = df_data['Metadata_Site'].astype(int).astype(str)\n",
    "# df_data['ImageID'] = df_data['Metadata_AcqID'] + '_' + df_data['Metadata_Well'] + '_' + df_data['Metadata_Site']\n",
    "# df_data['barcode'] = df_data['Metadata_Barcode']\n",
    "# df_data['well_id'] = df_data['Metadata_Well']\n",
    "# df_data['plate'] = df_data['Metadata_Barcode']\n",
    "# df_data['plate-name'] = df_data['Metadata_Barcode']\n",
    "# df_data['plateWell'] = df_data['Metadata_Barcode'] + '_' + df_data['Metadata_Well']\n",
    "# df_data['site'] = df_data['Metadata_Site']\n",
    "\n",
    "# display(df_data.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = df_concatenated_files.clone()\n",
    "    plate_names = data.select('Metadata_Barcode').unique().sort(by='Metadata_Barcode').to_series().to_list()\n",
    "    print(plate_names)\n",
    "except Exception:\n",
    "    print('Plate names not specified')\n",
    "    plate_names = []\n",
    "\n",
    "data = data.sort(['Metadata_Barcode','Metadata_Well', 'Metadata_Site'])\n",
    "\n",
    "wells = data.select('Metadata_Well').unique().sort(by='Metadata_Well').to_series().to_list()\n",
    "number_of_wells = len(wells)\n",
    "print(f'Number of wells: {number_of_wells}')\n",
    "\n",
    "rows = sorted(list({w[0] for w in wells}))\n",
    "number_of_rows = len(rows)\n",
    "print(*rows)\n",
    "\n",
    "columns = sorted(list({w[1:] for w in wells}))\n",
    "number_of_columns = len(columns)\n",
    "print(*columns)\n",
    "\n",
    "all_wells = [(x+y) for x in rows for y in columns]\n",
    "\n",
    "sites = data.select('Metadata_Site').unique().sort(by='Metadata_Site').to_series().to_list()\n",
    "number_of_sites = len(sites)\n",
    "print(f'Number of sites: {number_of_sites}')\n",
    "\n",
    "total_images = data.shape[0]\n",
    "expected_images = len(plate_names) * number_of_wells * number_of_sites\n",
    "\n",
    "print(f'Processed {total_images} of {expected_images} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Collect columns related to image quality\n",
    "image_quality_cols = [col for col in data.columns if \"ImageQuality_\" in col]\n",
    "\n",
    "# Remove 'ImageQuality_' prefix from column names\n",
    "image_quality_module = [col.replace('ImageQuality_', '') for col in image_quality_cols]\n",
    "\n",
    "# Get unique measures from column names, assuming measure is before first underscore\n",
    "image_quality_measures = sorted({re.sub('_.*', '', measure) for measure in image_quality_module})\n",
    "count_measures = len(image_quality_measures)\n",
    "\n",
    "print(f'Image Quality module has measured {count_measures} parameters: {\", \".join(image_quality_measures)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_so_useful = ['TotalArea', 'Scaling', 'TotalIntensity', 'Correlation', 'PercentMinimal',\n",
    "                 'LocalFocusScore', 'MinIntensity', 'MedianIntensity', 'MADIntensity',\n",
    "                 'ThresholdMoG', 'ThresholdBackground', 'ThresholdKapur', 'ThresholdMCT',\n",
    "                 'ThresholdOtsu', 'ThresholdRidlerCalvard', 'ThresholdRobustBackground',\n",
    "                 'PercentMaximal']\n",
    "\n",
    "image_quality_measures = [measure for measure in image_quality_measures if measure not in not_so_useful]\n",
    "count_measures = len(image_quality_measures)\n",
    "\n",
    "print(f'I will use {count_measures} parameters: {\", \".join(image_quality_measures)}')\n",
    "\n",
    "data_frame_dictionary = {measure: data[[col for col in image_quality_cols if f'_{measure}' in col]] for measure in image_quality_measures}\n",
    "data_frame_list = sorted(list(data_frame_dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation, LocalFocusScore, ThresholdMoG, ThresholdOtsu\n",
    "for i in range(22):\n",
    "    if len(data_frame_dictionary[data_frame_list[i]].columns) > 5:\n",
    "        print(i+1, data_frame_list[i], len(data_frame_dictionary[data_frame_list[i]].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = [\n",
    "    re.sub('.*_', '', c)\n",
    "    for c in list(data_frame_dictionary[data_frame_list[1]].columns)\n",
    "]\n",
    "channel_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "\n",
    "for plate in plate_names:\n",
    "    plate_data = data.filter(pl.col('Metadata_Barcode') == plate)\n",
    "    heatmap_data = []\n",
    "    for row in rows:\n",
    "        heatmap_row = []\n",
    "        for column in columns:\n",
    "            well = row + column\n",
    "            count_nuclei = plate_data.filter(pl.col('Metadata_Well') == well)['Count_nuclei'].to_numpy()\n",
    "            \n",
    "            # If the value is NaN, convert it to a specific value (like -1 or 0)\n",
    "            if count_nuclei.size == 0:\n",
    "                well_nuclei_count = 0  # Or whatever value you'd like to use for missing data\n",
    "            else:\n",
    "                well_nuclei_count = np.mean(count_nuclei).round(decimals = 0).astype(int)\n",
    "            \n",
    "            heatmap_row.append(well_nuclei_count)\n",
    "        heatmap_data.append(heatmap_row)\n",
    "\n",
    "    annotation_text = [[\"\" for _ in range(len(row))] for row in heatmap_data]\n",
    "    fig = ff.create_annotated_heatmap(heatmap_data, x=[i+1 for i in range(24)], y=rows,\n",
    "                                      annotation_text=annotation_text,\n",
    "                                      colorscale='OrRd', \n",
    "                                      hoverinfo='z')\n",
    "    fig.update_layout(\n",
    "        title_text=f'Plate: {plate}',\n",
    "        width=700)\n",
    "    fig.update_xaxes(side='bottom')\n",
    "    fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import plotly.subplots as sp\n",
    "\n",
    "# Define the number of columns for your grid\n",
    "plot_size = 400\n",
    "font_ratio = plot_size/400\n",
    "num_columns = 2\n",
    "num_rows = -(-len(plate_names) // num_columns)  # Ceiling division to get number of rows needed\n",
    "\n",
    "# Create a subplot with num_rows rows and num_columns columns\n",
    "fig = sp.make_subplots(rows=num_rows, cols=num_columns, subplot_titles=plate_names)\n",
    "\n",
    "for index, plate in enumerate(plate_names):\n",
    "    plate_data = data.filter(pl.col('Metadata_Barcode') == plate)\n",
    "    heatmap_data = []\n",
    "    for row in rows:\n",
    "        heatmap_row = []\n",
    "        for column in columns:\n",
    "            well = row + column\n",
    "            count_nuclei = plate_data.filter(pl.col('Metadata_Well') == well)['Count_nuclei'].to_numpy()\n",
    "            \n",
    "            if count_nuclei.size == 0:\n",
    "                well_nuclei_count = 0\n",
    "            else:\n",
    "                well_nuclei_count = np.mean(count_nuclei).round(decimals = 0).astype(int)\n",
    "            \n",
    "            heatmap_row.append(well_nuclei_count)\n",
    "        heatmap_data.append(heatmap_row)\n",
    "\n",
    "    # Calculate the subplot row and column indices\n",
    "    subplot_row = index // num_columns + 1\n",
    "    subplot_col = index % num_columns + 1\n",
    "    \n",
    "    heatmap = ff.create_annotated_heatmap(\n",
    "        heatmap_data,\n",
    "        x=[str(i+1) for i in range(24)],\n",
    "        y=rows,\n",
    "        annotation_text=heatmap_data,\n",
    "        colorscale='OrRd',\n",
    "        hoverinfo='z'\n",
    "    )\n",
    "\n",
    "    # Add the heatmap to the subplot\n",
    "    fig.add_trace(heatmap.data[0], row=subplot_row, col=subplot_col)\n",
    "\n",
    "# Update x and y axes properties\n",
    "for i in fig['layout']['annotations']:\n",
    "    i['font'] = dict(size=12*font_ratio)\n",
    "fig.update_xaxes(tickfont=dict(size=10*font_ratio), nticks=48, side='bottom')\n",
    "fig.update_yaxes(autorange=\"reversed\", tickfont=dict(size=10))\n",
    "# fig.update_yaxes(tickfont=dict(size=10*font_ratio))\n",
    "fig.update_layout(height=plot_size*num_rows, width=plot_size*1.425*num_columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Defining a color list\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "\n",
    "fig = sp.make_subplots(rows=len(image_quality_measures), cols=1, subplot_titles=image_quality_measures, x_title='Plates')\n",
    "\n",
    "for x in range(len(image_quality_measures)):\n",
    "    CurrentDataFrame = data_frame_dictionary.get(data_frame_list[x])\n",
    "    \n",
    "    min_val = CurrentDataFrame.min().to_numpy().min()  # minimum of all columns\n",
    "    max_val = CurrentDataFrame.max().to_numpy().max()  # maximum of all columns\n",
    "    for i, column in enumerate(CurrentDataFrame.columns):\n",
    "        channel_name = channel_names[i]\n",
    "        show_in_legend = (x == 0)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[str(j) for j in range(CurrentDataFrame.height)],\n",
    "                y=CurrentDataFrame[column],\n",
    "                mode='lines',\n",
    "                line=dict(width=0.5, color=colors[i % len(colors)]),\n",
    "                showlegend=False,\n",
    "                name=channel_name if not show_in_legend else \"\",\n",
    "                legendgroup=channel_name, \n",
    "            ),\n",
    "            row=x + 1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "    fig.update_xaxes(range=[0, CurrentDataFrame.height], showticklabels=False, row=x+1, col=1)\n",
    "\n",
    "    fig.add_shape(type=\"line\",\n",
    "        xref=\"x\", yref=\"paper\",\n",
    "        x0=CurrentDataFrame.height/2, y0=min_val, x1=CurrentDataFrame.height/2, y1=max_val,\n",
    "        line=dict(\n",
    "            color=\"Black\",\n",
    "            width=1,\n",
    "            dash=\"dashdot\",\n",
    "        ),\n",
    "        row=x + 1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "# Dummy traces for the legend\n",
    "for i, channel_name in enumerate(channel_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],  # these traces won't appear\n",
    "            y=[None],\n",
    "            mode='lines',\n",
    "            line=dict(width=3, color=colors[i % len(colors)]),  # this will be the width in the legend\n",
    "            legendgroup=channel_name,\n",
    "            name=channel_name,  # this will be the name in the legend\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Add main title\n",
    "fig.update_layout(height=1.5*len(image_quality_measures)*100, title_text=NameContains, title_x=0.1, width=1400)\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
